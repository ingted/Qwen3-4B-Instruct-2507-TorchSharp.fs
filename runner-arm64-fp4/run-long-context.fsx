#if INTERACTIVE
#load "loadCUDA.fsx"
#r "nuget: FAkka.TorchSharp.DGX, 26.1.0-py3.9"
#r "nuget: Tokenizers.DotNet, 1.3.0"
#r "nuget: Tokenizers.DotNet.runtime.linux-arm64, 1.3.0"
#r "/workspace/TorchSharp_In_DGX_Spark_fp4/TorchSharp.Q4.Extension/bin/Release/net10.0/TorchSharp.Q4.Extension.dll"
#r "/workspace/Qwen3-4B-Instruct-2507-TorchSharp.fs/bin/Release/net10.0/Qwen3-4B-Instruct-2507-TorchSharp.fs.dll"
#endif

open System
open System.IO
open System.Text
open System.Text.Json
open TorchSharp
open TorchSharp.Q4.Extension
open Qwen3_4B_Instruct_2507_TorchSharp_fs

// ==========================================
// åƒæ•¸è™•ç†
// ==========================================
let parseArgs (argv: string[]) =
    let rec loop i (m: Map<string, string>) =
        if i >= argv.Length then m
        else
            let key = argv.[i]
            if key.StartsWith("--") then
                if i + 1 < argv.Length && not (argv.[i + 1].StartsWith("--")) then
                    loop (i + 2) (m.Add(key, argv.[i + 1]))
                else
                    loop (i + 1) (m.Add(key, "true"))
            else
                loop (i + 1) m
    loop 0 Map.empty

let argMap = 
    #if INTERACTIVE
    parseArgs (fsi.CommandLineArgs |> Array.skip 1)
    #else
    parseArgs (Environment.GetCommandLineArgs() |> Array.skip 1)
    #endif

let tryGet key fallback = argMap |> Map.tryFind key |> Option.defaultValue fallback
let tryGetInt key fallback = argMap |> Map.tryFind key |> Option.map Int32.Parse |> Option.defaultValue fallback

let modelDir = tryGet "--model-dir" "/models/qwen3-4b-instruct-2507-torchsharp"
let weightPath = Some (tryGet "--weight" "Qwen3-4B-Instruct-2507-nvfp4.dat")
let targetTokens = tryGetInt "--target-tokens" 64000
let chunkSize = tryGetInt "--chunk-size" 4000 // æ¯ä¸€è¼ªæ³¨å…¥å¤šå°‘ tokens
let secretKey = tryGet "--secret" "GEMINI-CODE-7788"

printfn "ğŸš€ é•·ä¸Šä¸‹æ–‡æ¸¬è©¦å•Ÿå‹•"
printfn "ç›®æ¨™ Tokens: %d" targetTokens
printfn "å€å¡Šå¤§å°: %d" chunkSize
printfn "ç§˜å¯†é‡‘é‘°: %s" secretKey

// ==========================================
// åˆå§‹åŒ– Session
// ==========================================
let dtype = torch.ScalarType.Float16
let session = InferenceBridge.init modelDir weightPath (Some "fp4") "cuda" dtype

// æ¸…ç†å‡½æ•¸
let forceCleanUp () =
    System.GC.Collect()
    System.GC.WaitForPendingFinalizers()
    if torch.cuda_is_available() then
        torch.cuda.synchronize()
    NativeInterop.tryEmptyNvfp4Cache() |> ignore

let history = ResizeArray<string * string>()
let stopTokens = [ 151645; 151643 ]

let buildRenderedPrompt (history: ResizeArray<string * string>) (userMsg: string) =
    let sb = StringBuilder()
    for role, text in history do
        sb.Append("<|im_start|>").Append(role).Append('\n').Append(text).Append("<|im_end|>\n") |> ignore
    sb.Append("<|im_start|>user\n").Append(userMsg).Append("<|im_end|>\n") |> ignore
    sb.Append("<|im_start|>assistant\n") |> ignore
    sb.ToString()

let runTurn (userMsg: string) (maxTokens: int) (timeoutMs: int) =
    forceCleanUp()
    let prompt = buildRenderedPrompt history userMsg
    let currentTokens = session.Tokenizer.Encode(prompt).Length
    
    let opts: InferenceGenOptions =
        { MaxTokens = maxTokens
          Temperature = 0.0f
          TopP = 1.0f
          Seed = Some 123 }

    let sw = Diagnostics.Stopwatch.StartNew()
    let task = System.Threading.Tasks.Task.Run(fun () ->
        InferenceBridge.generateFromRenderedPromptWithStopTokensKvCache session prompt opts stopTokens KvPrefillMode.PromptByPrompt
    )
    
    if task.Wait(timeoutMs) then
        sw.Stop()
        let out = task.Result
        history.Add(("user", userMsg))
        history.Add(("assistant", out))
        printfn "[Turn] Context: %d tokens | Elapsed: %.1f ms" currentTokens sw.Elapsed.TotalMilliseconds
        out
    else
        printfn "âŒ è¶…æ™‚ä¸­æ–·ï¼ç›®å‰ Tokens: %d" currentTokens
        failwith "Timeout"

// ==========================================
// æ¸¬è©¦æµç¨‹
// ==========================================

try
    // 1. æ³¨å…¥ç§˜å¯†
    printfn "
1. æ³¨å…¥ç§˜å¯†è¨Šæ¯..."
    runTurn (sprintf "è«‹è¨˜ä½é€™å€‹ç§˜å¯†é‡‘é‘°ï¼Œä¹‹å¾Œæˆ‘æœƒå•ä½ ï¼š'%s'ã€‚é™¤äº†ç¢ºèªæ”¶åˆ°å¤–ï¼Œä¸è¦èªªå¤šé¤˜çš„è©±ã€‚" secretKey) 20 60000 |> ignore

    // 2. å¾ªç’°æ³¨å…¥å¡«å……æ–‡æœ¬
    printfn "
2. é–‹å§‹å¡«å……ä¸Šä¸‹æ–‡ (ç›®æ¨™: %d tokens)..." targetTokens
    let haystackSource = File.ReadAllText("haystack_source.txt")
    let mutable currentTotal = session.Tokenizer.Encode(buildRenderedPrompt history "").Length
    let mutable chunkCount = 1

    while currentTotal < targetTokens do
        // æ§‹é€ ä¸€å€‹å¤§ç´„ chunkSize å¤§å°çš„æ–‡æœ¬
        let sb = StringBuilder()
        while session.Tokenizer.Encode(sb.ToString()).Length < chunkSize do
            sb.Append(haystackSource).Append(" ") |> ignore
        
        let chunkText = sb.ToString()
        printfn "--- æ³¨å…¥å€å¡Š #%d ---" chunkCount
        // è¨­å®š MaxTokens = 1 æ¸›å°‘ç”Ÿæˆé–‹éŠ·ï¼Œæˆ‘å€‘åªæƒ³ prefill
        runTurn (sprintf "é€™æ˜¯åƒè€ƒè³‡æ–™ç¬¬ %d è™Ÿï¼š
%s" chunkCount chunkText) 1 120000 |> ignore
        
        currentTotal <- session.Tokenizer.Encode(buildRenderedPrompt history "").Length
        chunkCount <- chunkCount + 1

    // 3. æœ€çµ‚é©—è­‰
    printfn "
3. åŸ·è¡Œé‡å°–æ¸¬è©¦ (Needle Retrieval)..."
    let result = runTurn "æˆ‘ä¸€é–‹å§‹å«ä½ è¨˜ä½çš„ç§˜å¯†é‡‘é‘°æ˜¯ä»€éº¼ï¼Ÿè«‹ç›´æ¥å›ç­”é‡‘é‘°å…§å®¹ã€‚" 50 60000
    printfn "
[çµæœ] æ¨¡å‹çš„å›ç­”: %s" result
    
    if result.Contains(secretKey) then
        printfn "âœ… æ¸¬è©¦æˆåŠŸï¼æ¨¡å‹åœ¨ %d tokens ä¸‹ä¾ç„¶ä¿æœ‰æ­£ç¢ºè¨˜æ†¶ã€‚" currentTotal
    else
        printfn "âŒ æ¸¬è©¦å¤±æ•—ã€‚æ¨¡å‹å¯èƒ½éºå¿˜äº†é‡‘é‘°æˆ–å—åˆ°é•·æ–‡æœ¬å¹²æ“¾ã€‚"

finally
    InferenceBridge.dispose session
    printfn "æ¸¬è©¦çµæŸã€‚"
