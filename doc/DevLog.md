# Developer Log - 2026-02-14

## Issue: Intermittent Timeout at Stage [6] in `run-training2.fsx`

### 1. Troubleshooting
- **Symptoms**: Running `dotnet fsi run-training2.fsx --KVCacheOut false --no-kvc-mode full-replay --timing true` often hangs at stage `[6]`.
- **Observation**: While normal execution takes ~5s, it occasionally exceeds the 40s timeout. This usually happens during consecutive runs.
- **Environment**: DGX Spark (GB10) with 128GB Unified Memory (no HBM).
- **Hypothesis**: 
    - In `full-replay` mode, the prompt grows each turn, requiring larger contiguous memory for activation buffers.
    - Unified Memory fragmentation causes the CUDA allocator or OS to trigger memory compaction/page migration, leading to significant latency spikes.
    - Zombie `dotnet` processes from previous failed runs might be cluttering the system.

### 2. Change Planning
- **Goal**: Ensure clean memory state before the critical stage `[6]` and throughout the session.
- **Actions**:
    - Move `forceCleanUp()` (GC + CUDA Sync + Empty Cache) to the beginning of each `runTurn`.
    - Slightly increase the timeout for stage `[6]` to 48s (below the 50s system lockup threshold) to allow for minor OS-level memory management jitter.

### 3. Dev / Debug
- **Implementation**:
    - Relocated `forceCleanUp` definition above `runTurn`.
    - Inserted `forceCleanUp()` call at the start of `runTurn`.
    - Updated `runTurn (Some "6")` timeout from `40000` to `48000`.
- **Debugging**: Initially encountered `FS0039` error due to function definition order; fixed by moving the function block.

### 4. Test
- **Execution**: Ran a loop of 5 consecutive executions.
- **Result**: Successfully reproduced the hang *without* the fix in the loop. With the fix, stage `[6]` consistently completed in ~4.4s across multiple runs.

### 5. Solution Verificated
- **Status**: Verified. The combination of proactive GC and CUDA cache clearing before each turn prevents the accumulation of memory fragments that trigger the 40s+ latency in the GB10 Unified Memory architecture.
- **Final Logic**: Every turn now starts with a synchronized and defragmented memory state.

---

# Developer Log - 2026-02-24

## Objective
- Stabilize fp2 experiments to avoid host crash.
- Reproduce `!!!!` in single-turn mode only.
- Prepare A/B/C first-token diagnostics for weight/tokenizer alignment analysis.

## Changes Implemented
1. Added `run-training-fp2-safe.fsx`.
   - Based on `run-training-fp2-single.fsx`.
   - Single-turn only.
   - Fail-fast when first output contains `!!!!`.
   - Requires `TS_Q4_STE_USE_NATIVE_QUANTIZE=1`; otherwise exits immediately.
   - Safety cap: `--max-tokens <= 8`.
   - Default log path moved to:
     - `alpha/log/tee-object-chat-session-fp-safe.txt`
     - `alpha/log/tee-object-chat-session-fp-safe.jsonl`
2. Added `run-training-fp2-noste.fsx`.
   - Single-turn only.
   - Uses `Qwen3Core.forwardBlockNoCache` with `InferenceBridge.linearQ4` projections.
   - Purpose: no-STE control path while keeping block-graph style execution.
3. Added `compare-first-token-fp2.fsx`.
   - One-prompt first-token diagnostic.
   - Compares three paths:
     - `A.infer`: `InferenceBridge.forwardModel`
     - `B.fp2_ste`: `Qwen3Model.forward` (training/STE path)
     - `C.noste_graph`: block graph + `linearQ4` (no STE)
   - Prints hidden/logits NaN/Inf health + top10 token ids and decoded token text.

## Key Findings (from successful prior CUDA window)
1. `!!!!` corresponds to repeated token id `0`.
   - Evidence: `QWEN3_FS_DEBUG_TOKENS=1` showed `[0; 0; 0; 0]` in first turn.
   - Tokenizer check:
     - `decode(0) = "!"`
     - `encode("!") = [0]`
2. `debug-fp2-parity.fsx` indicated divergence at layer 0.
   - Path A healthy, Path B goes NaN from layer0 onward.
   - `q/k/v` in Path B had much larger absolute magnitude than Path A.

## Runtime Incident During This Session
1. `run-training-fp2-safe.fsx` attempt failed at CUDA init:
   - `cudaGetDeviceCount Error 304: OS call failed or operation not supported on this OS`
   - `Torch device type CUDA did not initialise on the current machine`
2. Confirmed this was environment-state dependent:
   - `nvidia-smi` initially looked normal.
   - Python check in same container then reported:
     - `torch.cuda.is_available() = False`
     - `torch.cuda.device_count() = 0`
   - Therefore this roundâ€™s fp2 runs were blocked by CUDA runtime availability, not script syntax.

## Commands Used (representative)
1. Safe run:
   - `TS_Q4_STE_USE_NATIVE_QUANTIZE=1 QWEN3_FS_DEBUG_TOKENS=1 timeout 120s dotnet fsi run-training-fp2-safe.fsx --prompt "hi" --max-tokens 4 --timing true`
2. CUDA status check:
   - `nvidia-smi`
   - `python3 - <<'PY' ... torch.cuda.is_available()/device_count ... PY`

## Next Action (once CUDA becomes available again)
1. Re-run `run-training-fp2-safe.fsx` (single-turn).
2. Run `run-training-fp2-noste.fsx` with same prompt.
3. Run `compare-first-token-fp2.fsx` and persist top10 diff in log.
4. Prioritize fixes in STE path (`linearSte/steWeight`) if A/C align but B diverges.

## Execution Update - 2026-02-24 (CUDA available window)
### Experiment 1: `run-training-fp2-safe.fsx`
- Command:
  - `TS_Q4_STE_USE_NATIVE_QUANTIZE=1 QWEN3_FS_DEBUG_TOKENS=1 timeout 180s dotnet fsi run-training-fp2-safe.fsx --prompt "hi" --max-tokens 4 --timing true`
- Result:
  - output: `!!!!`
  - generated ids: `[0; 0; 0; 0]`
  - process exited by guard fail-fast (`first output is !!!!`)
  - VRAM peak observed by watchdog: ~93.5GB (under 110GB kill threshold)

### Experiment 2: `run-training-fp2-noste.fsx`
- Initial run failed with missing assembly reference (`TorchSharp.Fun.DGX`).
- Fix:
  - Added `#r "/workspace/TorchSharp.Fun.DGX/TorchSharp.Fun.DGX/bin/Release/net10.0/TorchSharp.Fun.DGX.dll"`.
- Command:
  - `QWEN3_FS_DEBUG_TOKENS=1 timeout 180s dotnet fsi run-training-fp2-noste.fsx --prompt "hi" --max-tokens 4 --timing true`
- Result:
  - generated ids: `[13048; 0; 26525; 232]`
  - output: `Hi! ğŸ˜Š`
  - VRAM peak: ~5.4GB

### Experiment 3: `compare-first-token-fp2.fsx`
- Initial compile fixes:
  1. `torch.topk` argument type fixes (`int` vs `int64`).
  2. Added `TorchSharp.Fun.DGX` reference.
- Command:
  - `TS_Q4_STE_USE_NATIVE_QUANTIZE=1 timeout 240s dotnet fsi compare-first-token-fp2.fsx "hi"`
- Key observations:
  - `A.infer`: hidden/logits finite, top1 token `id=2132 ("It")`.
  - `B.fp2_ste`: hidden/logits contain NaN; top10 all NaN logits with low-id punctuation (`id=0` included).
  - `C.noste_graph`: hidden/logits finite; top10 highly similar to `A.infer`.
- VRAM peak: ~83.8GB (under threshold).

### Conclusion
- Root-cause is now strongly isolated to STE path (`Qwen3Model.forward` + `Nvfp4Training.linearSte`), not tokenizer and not block-graph wiring itself.

### Re-run Confirmation (prompt=`"hi"`, after arg parsing fix)
- `compare-first-token-fp2.fsx "hi"` now correctly uses prompt `hi`.
- A/C top tokens are highly aligned and semantically correct:
  - top1 both are `id=13048 ("Hi")`, followed by `Hello/Hey/HI/...`.
- B remains invalid:
  - hidden/logits contain NaN
  - top10 collapses to low-id punctuation tokens with NaN logits.
- This removes prior ambiguity from incorrect prompt parsing and further confirms STE-specific failure.

## Fix Execution - 2026-02-24 (STE recovery)
### Root-cause evidence added
- Parsed dat entries and confirmed:
  - `*.qdata`: `elemType=0`
  - `*.scale`: `elemType=101` (1-byte encoded scale), shape `[out, in/16]`

### Code fix
- File: `Qwen3-4B-Instruct-2507-TorchSharp.fs/Qwen3Model.fs`
- Change:
  - In `materializeMasterWeight`, when `scale.dtype = uint8`, decode scale bytes as FP8(E4M3FN) to float first, then call `Nvfp4Training.dequantizePacked`.
  - Added LUT-based decode helper (`fp8E4M3FnToFloat32`, `decodeFp8E4M3FnTensor`).

### Validation after fix
1. `run-training-fp2-safe.fsx --prompt "hi"`:
   - generated ids: `[9707; 0; 61804; 233]`
   - output: `Hello! ğŸ‘‹`
   - no `!!!!` fail-fast triggered
2. `compare-first-token-fp2.fsx "hi"`:
   - `A.infer`: finite, top1 `Hi`
   - `B.fp2_ste`: finite (no NaN), top tokens `Hello/Hi/Hey/...`
   - `C.noste_graph`: finite, aligned with `A`

### Decision
- Keep single-turn guardrails for stability, but STE path is no longer in NaN-collapse state for the tested prompt.

## Regression Check - 2026-02-24 (run-training-fp2 main script)
### Command
- `cd /workspace/fsann/alpha/runner-arm64-fp4`
- `TS_Q4_STE_USE_NATIVE_QUANTIZE=1 QWEN3_FS_DEBUG_TOKENS=1 dotnet fsi run-training-fp2.fsx --max-tokens 4 --timing true --check-logits false --prompt "hi"`

### Result
- Script completed in single-turn guard mode.
- generated ids: `[9707; 0; 61804; 233]`
- output: `Hello! ğŸ‘‹`
- no `!!!!` fail-fast triggered.

### Runtime note
- On this host/GPU, `nvidia-smi` reports `Memory-Usage: Not Supported`, so the external `>110GB for 10s` VRAM kill rule cannot be enforced via NVML query in this environment.
- Effective fallback during this run: strict single-turn mode + script fail-fast + command timeout pattern.

## Guard Script - 2026-02-24
### Goal
- Add a reusable launcher that enforces the user's GPU safety rule from `nvidia-smi` process table (`GPU Memory`) when available.

### Implementation
- Added: `run-training-fp2-guarded.sh`
  - Runs `dotnet fsi run-training-fp2.fsx`.
  - Watches the target PID via:
    - `nvidia-smi --query-compute-apps=pid,used_memory`
    - fallback: parse `nvidia-smi` processes table text.
  - If memory is over 110GB for 10s (configurable), sends `TERM` then `KILL`.
  - Defaults:
    - `TS_Q4_STE_USE_NATIVE_QUANTIZE=1`
    - `QWEN3_FS_DEBUG_TOKENS=1`
    - args: `--max-tokens 4 --timing true --check-logits false --prompt hi`

### Validation
- Command:
  - `timeout 150s ./run-training-fp2-guarded.sh --max-tokens 1 --timing false --check-logits false --prompt "hi"`
- Result:
  - training script completed and output remained normal (`Hello`).
  - No watchdog-triggered kill.

### Caveat observed in this runtime
- For this run, process-level GPU memory stayed `0MiB` in both query/fallback paths.
- Added explicit warning in script:
  - if memory remains unobservable for >=15s, report that threshold enforcement is currently unavailable.

## Usability Fix - 2026-02-24 (no-env run)
### Problem
- `dotnet fsi run-training-fp2.fsx` failed early when `TS_Q4_STE_USE_NATIVE_QUANTIZE` was not pre-set.

### Change
- File: `run-training-fp2.fsx`
- Replaced hard fail check with auto-enforcement:
  - if env var is missing/false, script sets `TS_Q4_STE_USE_NATIVE_QUANTIZE=1` at startup and logs an info line.

### Intent
- Keep OOM safety behavior while allowing direct invocation without requiring users to export env vars manually.

## Default Args Fix - 2026-02-24
### Problem
- Running `dotnet fsi run-training-fp2.fsx` without args still failed safety cap because default `--max-tokens` was `20`.

### Change
- File: `run-training-fp2.fsx`
- Updated default `MaxTokens` from `20` to `8` to match fp2-safe cap.
- Also updated `defaultArgs` (`--max-tokens`) from `20` to `8` because no-arg invocation uses `defaultArgs`.

## Multi-turn Enablement - 2026-02-24
### Request
- Remove hard single-turn token cap behavior and support multi-turn scenario similar to `run-training2.fsx`.

### Change
- File: `run-training-fp2.fsx`
1. Removed hard fail gate `if MaxTokens > 8 then fail`.
2. Added args:
   - `--turns` (default: `1`)
   - `--followup-prompt` (default: `continue.`)
3. Runtime now executes turns in loop:
   - turn1 uses `--prompt`
   - turn2..N use `--followup-prompt`
4. Kept guard behavior:
   - any turn output containing `!!!!` triggers immediate fail-fast.

### Smoke test status
- Command attempted:
  - `dotnet fsi run-training-fp2.fsx --turns 2 --prompt "hi" --followup-prompt "continue" --max-tokens 2 --timing false --check-logits false --stop-here false`
- In this execution context, run blocked by CUDA init failure before generation (`Torch device type CUDA did not initialise`), so multi-turn runtime output must be confirmed on stable CUDA session.

## No-Arg Defaults Update - 2026-02-24
- run-training-fp2.fsx default profile adjusted for zero-arg usage:
  - --turns=3
  - --prompt=hi
  - --followup-prompt=continue.
  - --max-tokens=8
- Goal: allow multi-turn regression runs without bash parameters.

## Default Prompt Alignment - 2026-02-24
- Restored no-arg default `--prompt` to: "Write one short sentence about UFO and you." for parity with run-training2.fsx comparisons.

## Repro Profile Reset - 2026-02-24
### Problem
- No-arg multi-turn defaults can push VRAM too high before first valid output is observed.

### Change
- File: `run-training-fp2.fsx`
- Reset zero-arg defaults to one-shot reproducibility profile:
  - `--turns=1`
  - `--max-tokens=4`
  - `--prompt="Write one short sentence about UFO and you."` (kept for parity with `run-training2.fsx`)

### Expected behavior
- `dotnet fsi run-training-fp2.fsx` should prioritize producing one valid first output.
- Multi-turn remains available via explicit `--turns` override.

## Guard Runner Update - 2026-02-25
### Request
- Stop using bash guard wrapper and use F# guard runner only.
- Print dotnet process ID clearly so operator can kill quickly.

### Code review findings (`run-script-with-guard.fsx`)
1. Missing positive-value validation for guard parameters.
2. Missing explicit script existence check.
3. PID visibility can be improved (guard PID + child dotnet PID).

### Fixes
1. Added validation: `--gpu-limit-gb`, `--gpu-over-secs`, `--gpu-poll-secs` must all be `> 0`.
2. Added script path existence check before spawn.
3. Added explicit logs:
   - `[guard] guard_pid=<pid>`
   - `[guard] started dotnet_pid=<pid>`
4. Added compatibility normalization when positional args accidentally include leading `script`.

## Guard Tuning - 2026-02-25 (110GB immediate mode)
### Why 117GB still happened previously
- Previous run used `--gpu-over-secs 10` semantics, so crossing threshold did not kill immediately.
- With 115GB limit, observed 117479MiB was still below 117760MiB limit.

### Changes
- `run-script-with-guard.fsx` now supports fractional `--gpu-poll-secs` / `--gpu-over-secs`.
- Defaults changed to safer profile:
  - `gpu-limit-gb=110`
  - `gpu-over-secs=0` (immediate kill)
  - `gpu-poll-secs=0.5`
- Guard checks both target PID memory and total GPU process memory.
- Kill log now prints `pid_mem/total_mem/limit` for postmortem.

### Critical bug fixed
- `over-secs=0` initially caused unconditional kill due branch condition.
- Fixed to only use sustained-window branch when `over-secs > 0`.

### Verification command
- `dotnet fsi run-script-with-guard.fsx --gpu-limit-gb 110 --gpu-over-secs 0 --gpu-poll-secs 0.5 script run-training-fp2.fsx --max-tokens=8 --timing=true --check-logits=false --stop-here=false`
- Result: guard killed process in immediate mode when threshold was crossed.

## Training Path Hardening - 2026-02-25 (fp2-model only)
### Goal
- Stop using bridge inference path in `run-training-fp2.fsx`.
- Make training-path inference stable and reproducible for full pure-NVFP4 training bring-up.

### Root-cause refinement
1. `fp2-model` path still had startup residency pressure because runner first called full `InferenceBridge.init` (loads all layer Q4 objects), then attempted to dispose layers.
2. `linearSte` repeatedly quantize/dequantize weights in eval decode loop; this creates unnecessary temporary pressure and allocator growth.

### Code changes
1. `TorchSharp.Q4.Extension/Nvfp4Training.fs`
   - Added eval-only STE weight cache (enabled by `TS_Q4_STE_CACHE_EVAL_WEIGHT=1`).
   - Added `clearEvalWeightCache()`.
   - Fixed temporary input tensor ownership/dispose in `linearSte` when dtype cast occurs.
2. `TorchSharp.Q4.Extension/Nvfp4Training.fsi`
   - Exported `clearEvalWeightCache`.
3. `Qwen3-4B-Instruct-2507-TorchSharp.fs/InferenceBridge.fs`
   - Added `initSamplingOnly` (loads only tokenizer/embed/final_norm/lm_head).
4. `run-training-fp2.fsx`
   - Default backend switched to `fp2-model`.
   - `bridge` backend is now rejected for this runner (training-path-only policy).
   - Uses `InferenceBridge.initSamplingOnly` for fp2 backend.
   - Enforces `NVFP4_quantize` export availability; fail if missing.
   - Auto-enables:
     - `TS_Q4_STE_USE_NATIVE_QUANTIZE=1`
     - `TS_Q4_STE_CACHE_EVAL_WEIGHT=1`
   - Clears eval cache in `finally`.
   - Default `--max-tokens` raised to `24` for no-arg full-sentence output.

### Build verification
1. `cd /workspace/TorchSharp_In_DGX_Spark_fp4/TorchSharp.Q4.Extension && dotnet build -c Release`
   - pass
2. `cd /workspace/Qwen3-4B-Instruct-2507-TorchSharp.fs && dotnet build -c Release`
   - pass

### Experiments (guarded)
1. Command:
   - `cd /workspace/fsann/alpha/runner-arm64-fp4`
   - `dotnet fsi run-script-with-guard.fsx --gpu-limit-gb 108 --gpu-over-secs 0 --gpu-poll-secs 0.5 script run-training-fp2.fsx`
2. Result:
   - completed, no watchdog kill.
   - peak `total_gpu_mem` observed around `44GB`.
   - output:
     - `Iâ€™ve never seen a UFO, but Iâ€™ve always wondered what it would be like to meet one.`
3. Additional:
   - `--max-tokens=24` explicit run also passed under same guard.

### Experiments (direct no guard)
1. Command:
   - `dotnet fsi run-training-fp2.fsx`
2. Result:
   - completed with coherent sentence output.
   - no `!!!!` collapse.

## Persistent Multi-turn KVC - 2026-02-25
### Goal
- Complete multi-turn chat continuation for training path (`fp2-model`) with real KV reuse across turns.

### Implementation
1. File: `run-training-fp2.fsx`
   - Added shared persistent objects:
     - `fp2PersistentCache : ModelKvCache option`
     - `fp2PersistentContextTokens : ResizeArray<int>`
   - Added `generateFromUserMessageWithStopTokensFpKvPersistent`.
2. Protocol change
   - Old:
     - each turn built full rendered prompt and rebuilt/reprefilled cache.
   - New:
     - each turn encodes only current user turn prefix.
     - decode with persistent cache.
     - every accepted token is forwarded into cache immediately.
     - append/prefill `<|im_end|>\n` after each turn to keep template alignment.
3. Added debug observability:
   - `[FP2Debug] kvc seqLen=... contextTokens=...`
   - `[kvc] turn-cache seqLen=... contextTokens=...`

### Guarded multi-turn test (speed/continuation proof)
1. Command:
   - `dotnet fsi run-script-with-guard.fsx --gpu-limit-gb 108 --gpu-over-secs 0 --gpu-poll-secs 0.5 script run-training-fp2.fsx --turns=3 --max-tokens=8 --timing=true --check-logits=false --stop-here=false`
2. Result:
   - no watchdog kill.
   - seqLen/context grew:
     - turn1: `27`
     - turn2: `47`
     - turn3: `67`
   - generation latency:
     - turn1: ~`4.1s`
     - turn2: ~`0.54s`
     - turn3: ~`0.55s`
3. Interpretation:
   - turn2/3 are reusing KV instead of replaying full history.

### Guarded semantic continuation test
1. Command:
   - `dotnet fsi run-script-with-guard.fsx --gpu-limit-gb 108 --gpu-over-secs 0 --gpu-poll-secs 0.5 script run-training-fp2.fsx --turns=2 --max-tokens=12 --prompt=\"Write one short sentence about UFO and you.\" --followup-prompt=\"continue the previous sentence in one clause.\" --timing=true --check-logits=false --stop-here=false`
2. Result:
   - turn1: `Iâ€™ve never seen a UFO, but Iâ€™ve always wondered`
   - turn2: `if I ever do, Iâ€™ll know itâ€™s not a`
3. Interpretation:
   - turn2 continues prior clause; multi-turn context carry is effective.

## KVC Workstart - 2026-02-25
### Context and hypothesis
1. `max-tokens=4` can finish, but `6/8/10` frequently cross 110~117GB.
2. Current fp2 generation path still had full-replay behavior before this wave.
3. Additional memory pressure also came from loading two full model families in the same process:
   - `InferenceBridge` full layers
   - `Qwen3Model` full blocks

### SD-driven implementation done
1. `run-training-fp2.fsx` adds `--use-kvc` (default: `true`).
2. Added KV decode path:
   - prefill once with full prompt via `Qwen3Model.forwardWithKvCache`.
   - incremental decode using one token per step with the same cache.
3. Kept replay path for fallback (`--use-kvc false`).
4. Inference mode hardening:
   - generation switched from `torch.no_grad()` to `torch.inference_mode()`.

### Memory pressure reduction done
1. Added selective disposal of `InferenceBridge` per-layer weights right after init in `use-kvc=true` mode.
2. Keep only tokenizer + embedding + final norm + lm head from `InferenceBridge` for sampling.
3. Finalizer path updated to avoid double-dispose and still release kept components.

### Guard policy update
1. Default guard limit changed to `108GB`.
2. Tests run with:
   - `--gpu-limit-gb 108 --gpu-over-secs 0 --gpu-poll-secs 0.5`
3. All tests executed through `run-script-with-guard.fsx` (no timeout path).

### Experiments (post-KVC start)
1. Command:
   - `dotnet fsi run-script-with-guard.fsx --gpu-limit-gb 108 --gpu-over-secs 0 --gpu-poll-secs 0.5 script run-training-fp2.fsx --max-tokens=4 --timing=true --check-logits=false --stop-here=false`
   - Result: PASS, output `Iâ€™ve never seen`.
2. Command:
   - same but `--max-tokens=8`
   - Result: KILLED by guard, breach log:
     - `total_mem=112791MiB`, `limit=110592MiB`.
3. Command:
   - same but `--max-tokens=6`
   - Result: KILLED by guard, breach log:
     - `total_mem=113269MiB`, `limit=110592MiB`.

### Current read
1. Guard works as intended now (108GB + 0.5s + immediate kill).
2. KVC first pass is integrated, but peak memory is still above 108GB for `max-tokens>=6`.
3. Next action:
   - continue profiling around decode-step peak and temporary buffers in STE linear path.

## KVC Backend Split - 2026-02-25 (stability-first)
### Design decision
1. Add `--kvc-backend` in `run-training-fp2.fsx`:
   - `bridge` (default): use `InferenceBridge` KVC generation.
   - `fp2-model`: use `Qwen3Model.forwardWithKvCache` path.
2. Keep `fp2-model` for parity/debug, but use `bridge` as default to guarantee practical output and safe VRAM.

### Why
1. `fp2-model` KVC path under strict `108GB` still breached at `max-tokens=6/8`.
2. `bridge` KVC path avoids duplicate heavy residency and has much lower decode peak.

### Key implementation notes
1. `run-training-fp2.fsx` now conditionally skips `Qwen3Model.create` when `--use-kvc=true --kvc-backend=bridge`.
2. In bridge mode, generation calls:
   - `InferenceBridge.generateFromRenderedPromptWithStopTokensKvCache ... KvPrefillMode.PromptByPrompt`
3. `check-logits` in bridge mode uses `InferenceBridge.checkLogits`.

### Verification (guard=108GB, over=0, poll=0.5)
1. `--max-tokens=8`:
   - output: `I once saw a UFOâ€”no,`
   - peak sample log: `total_gpu_mem=6053MiB`
   - PASS (no kill)
2. `--max-tokens=10`:
   - output: `I once saw a UFOâ€”no, just a`
   - PASS (no kill)
3. `--max-tokens=16`:
   - output: `I once saw a UFOâ€”no, just a bright light in the sky,`
   - PASS (no kill)
4. `--max-tokens=24`:
   - output: `I once saw a UFOâ€”no, just a bright light in the sky, maybe a plane or a satellite. ï¿½`
   - PASS (no kill)

### Additional fp2-model check (after cleanup tweaks)
1. Command:
   - `dotnet fsi run-script-with-guard.fsx --gpu-limit-gb 108 --gpu-over-secs 0 --gpu-poll-secs 0.5 script run-training-fp2.fsx --use-kvc=true --kvc-backend=fp2-model --max-tokens=6 --timing=true --check-logits=false --stop-here=false`
2. Result:
   - still KILLED at threshold
   - breach log: `total_mem=112051MiB`, `limit=110592MiB`
3. Interpretation:
   - bridge backend is now the only stable default for long-enough outputs under 108GB.
   - fp2-model backend remains a dedicated optimization/parity track.

## Discussion Record - 2026-02-25 (user Q&A summary)
### 1) What is `eval cache` and `clear`?
1. `eval cache`:
   - cache dequantized STE weight for `Nvfp4Training.linearSte` during `inference_mode/no_grad`.
   - avoid repeated `quantize -> dequantize` per token on the same layer weight.
2. `clear`:
   - `Nvfp4Training.clearEvalWeightCache()` explicitly disposes cached tensors.
   - called in runner `finally` to avoid cache residue across runs.

### 2) Is this caused by .NET itself?
1. Conclusion:
   - not a `.NET` root-cause issue.
2. Reason:
   - pressure comes from algorithmic path cost (STE repeated quant/dequant in decode loop).
   - .NET/TorchSharp requires explicit tensor lifecycle control, so disposal mistakes amplify allocator pressure.

### 3) Can current VRAM do prompt-by-prompt KVC?
1. For current fp2 inference-style path (`run-training-fp2.fsx`):
   - yes, prompt-by-prompt/persistent KVC is working under guard.
   - observed peak around ~44GB in guarded runs.
2. For true training (backward + optimizer):
   - not equivalent to inference footprint.
   - feasibility must be validated by dedicated train-step profiling (activation + optimizer states can dominate).

## Full-NVFP4 1-step Training Bring-up - 2026-02-25
### Goal
1. Add real 1-step training execution on training path (not inference path), with:
   - loss/backward/optimizer-step
   - gradient-checkpoint style recompute control
   - optimizer state compression/offload controls
   - guarded VRAM profiling

### New script
1. Added `run-train-step-full-nvfp4.fsx`:
   - loads training-path model graph.
   - executes one real train step (`forward -> loss -> backward -> step`).
   - supports `--grad-ckpt-chunk` for chunked recompute behavior.
   - emits phase VRAM samples and optional JSON report via `--vram-report`.

### Optimizer/state implementation
1. Implemented packed NVFP4 persistent state (`w/m/v`) in script-local optimizer flow:
   - persistent storage as packed `qdata + scale`.
   - unpack/materialize only when needed for update.
2. Added memory control toggles:
   - `--offload-mv-to-cpu`
   - `--offload-w-to-cpu`
   - `--offload-grad-to-cpu`
   - `--step-flush-each-param`
   - `--materialize-from-packed` (default false)
   - `--compute-grad-norm` (default false)

### Guarded experiments
1. All runs used guard launcher (no timeout path):
   - `dotnet fsi run-script-with-guard.fsx --gpu-limit-gb 108 --gpu-over-secs 0 --gpu-poll-secs 0.5 script run-train-step-full-nvfp4.fsx ...`
2. Main observed phase memory:
   - `model_loaded`: ~40065MiB
   - `state_initialized`: ~40185MiB
   - `backward_done`: ~82602MiB
   - optimizer-step peak: ~110.9GiB to ~117.5GiB (varies by options), often guard-killed.
3. With `--offload-grad-to-cpu=true`, a long run ended with exit code `137` before final report write (likely external/system kill during/near step phase).

### Current conclusion
1. Inference KVC stability does not translate to training-step VRAM safety.
2. Current bottleneck is optimizer-step transient peak (not only steady grad footprint).
3. Under strict 108GB immediate guard, 1-step full-NVFP4 training is not yet stable.

### Next optimization direction
1. Reduce step transients with stricter per-parameter streaming update.
2. Shorten guard reaction window further (poll interval reduction where possible).
3. Keep all future train-step experiments under `run-script-with-guard.fsx` only.

## Full-NVFP4 Train-step Round 2 - 2026-02-25 (diagnostics + chunked step)
### Scope
1. Add requested diagnostics:
   - process memory telemetry per phase.
   - model/state tensor byte breakdown for duplicate/multi-copy detection.
2. Optimize `model_loaded` and `optimizer step` transient memory.
3. Keep all runs under guard launcher.

### Code changes (`run-train-step-full-nvfp4.fsx`)
1. Added phase telemetry fields:
   - `pid_mem_mib`, `total_gpu_mem_mib`, `cuda_used_mib`, `cuda_total_mib`, `proc_rss_mib`.
2. Added `cudaMemGetInfo` bridge via `libcudart` P/Invoke.
3. Added tensor byte summary helper:
   - grouped by `kind/device/dtype`.
4. Added runtime controls:
   - `--step-chunk-rows` (default now `32`)
   - `--dispose-session-after-load` (default `true`)
   - `--compact-after-model-load` (default `true`)
   - `--print-tensor-byte-report` (default `true`)
   - `--stop-after` (phase stop for diagnostics).
5. Optimizer redesign:
   - `adamwStepNvfp4Packed` switched to row-chunk streaming.
   - update no longer requires full-parameter simultaneous dequant/materialization.

### Diagnostic findings
1. Command:
   - `dotnet fsi run-script-with-guard.fsx --gpu-limit-gb 108 --gpu-over-secs 0 --gpu-poll-secs 0.5 script run-train-step-full-nvfp4.fsx --seq-len=1 --grad-ckpt-chunk=0 --stop-after=model_loaded ...`
2. Key output:
   - `model_loaded`: `pid=40065MiB`, `cuda_used=~52GiB`.
3. With `--stop-after=post_load_compacted`:
   - `model_loaded`: `40065MiB`
   - `post_load_compacted`: `38303MiB`
4. Interpretation:
   - load-time compaction yields ~`1.7GiB` reduction on observed process footprint.

### Model/state byte breakdown
1. Command:
   - `... --stop-after=state_initialized --print-tensor-byte-report=true`
2. Key output:
   - `model.parameters(unique)`: `6930.37 MiB` (`Float16`, `cuda:0`, 396 params)
   - packed state total (`w/m/v`): `5847.50 MiB` (stored on CPU)
   - no duplicate parameter refs (`raw=396, unique=396`)
3. Interpretation:
   - weight payload itself is ~6.9GiB; `model_loaded ~40GiB` mostly includes runtime/allocator/workspace.

### Chunked optimizer experiments
1. `step-chunk-rows=32` (guard 108GB):
   - completed one full step in an earlier run.
   - observed peak stayed under guard (highest sampled total around `~104.6GiB` in that run).
   - produced:
     - `optimizer_step_done`
     - final `[done]` marker
     - VRAM report file written.
2. `step-chunk-rows=64` (guard 108GB):
   - failed with CUDA OOM in `NVFP4_quantize` path.
   - notable runtime message:
     - allocated ~`101.31GiB`, reserved ~`2.25GiB`, requested extra `20MiB`.
3. Decision:
   - set default `step-chunk-rows=32` (memory-first default).

### Additional observations
1. `backward_done` on current script path (seq=1):
   - repeatedly observed near `52024MiB`.
2. `grad_offload_done` can increase host RSS significantly (expected CPU clone cost).
3. Some long stress runs ended with exit `137` despite guard not triggering threshold kill.
   - treated as external/system kill risk under prolonged high-pressure allocator state.

### Attempted change and rollback
1. Tried deferring packed-state initialization to after backward/offload.
2. Result:
   - one run showed lower intermediate peak, but another run entered long unstable/stalled behavior.
3. Action:
   - rolled back defer-init ordering to stable path.
   - retained proven improvements (diagnostics, load compaction, chunked streaming step).

## 2026-02-25 (A~F implementation pass on project branch)
### Scope
1. å›ç­”ä¸¦è½åœ° A~Fï¼š
   - A/B é‡æ¸…ï¼ˆcheckpointing/KVC/GQA/offloadï¼‰
   - C å»ºç«‹è¨“ç·´æ–‡æœ¬è³‡æ–™
   - D æ–‡ä»¶å›é·åˆ°å°ˆæ¡ˆ `doc/`
   - E/F è£œè¨“ç·´å•Ÿå‹•æ–¹å¼ã€Trainer JSON VRAM reportã€æœ€å° 1-step å¯¦è¨“è…³æœ¬

### Changes
1. `Types.fs`
   - æ–°å¢ `TrainStepVramReportPath`ã€‚
   - å°‡ `OffloadMVToCpu/OffloadWToCpu/OffloadGradToCpu` é è¨­æ”¹ç‚º `false`ã€‚
2. `Cli.fs`
   - æ–°å¢ `--train-step-vram-report <path>`ã€‚
3. `Program.fs`
   - init log æ–°å¢ `vramReport` é¡¯ç¤ºã€‚
4. `Trainer.fs`
   - æ–°å¢ `TrainVramSample/TrainVramReport`ã€‚
   - phase æ¡æ¨£æ”¹ç‚ºå¯åŒæ™‚åš console + JSONã€‚
   - finally éšæ®µè¼¸å‡º JSON å ±è¡¨ã€‚
5. `TrainData/`
   - æ–°å¢ `train-inputs.txt`ï¼ˆ10 çµ„çŸ­æ–‡æœ¬ï¼‰èˆ‡ `README.md`ã€‚
6. `scripts/Train.OneStep.fsx`
   - æ–°å¢æœ€å°å¯é‡ç¾ 1-step å¯¦è¨“è…³æœ¬ã€‚
   - ç›´æ¥èµ°å°ˆæ¡ˆ APIï¼š`Qwen3Model`, `InferenceBridge`, `Trainer.scalarLoss`, `Nvfp4Optimizer`ã€‚
   - å…§å»º phase VRAM æ¡æ¨£èˆ‡ JSON å ±å‘Šè¼¸å‡ºã€‚
7. æ–‡ä»¶å›é·
   - runner `SA/SD/DevLog/WBS` åŒæ­¥è‡³å°ˆæ¡ˆ `doc/`ã€‚
   - å¾ŒçºŒä»¥å°ˆæ¡ˆ `doc/` ç‚ºä¸»ã€‚

### Validation
1. æœ¬è¼ªå…ˆå®Œæˆéœæ…‹æ•´åˆï¼Œå¾… build é©—è­‰ã€‚
2. è‹¥ build é€šéï¼Œå†äº¤ä»˜è¨“ç·´å•Ÿå‹•å‘½ä»¤èˆ‡ guard å»ºè­°ã€‚

### Build result
1. Command:
   - `dotnet build Qwen3-4B-Instruct-2507-TorchSharp.fs.fsproj -c Release`
2. Result:
   - `Build succeeded` (warnings only, no errors).

### Guarded run (one-step text training)
1. Command:
   - `dotnet fsi /workspace/fsann/alpha/runner-arm64-fp4/run-script-with-guard.fsx --gpu-limit-gb 108 --gpu-over-secs 0 --gpu-poll-secs 0.5 script /workspace/Qwen3-4B-Instruct-2507-TorchSharp.fs/scripts/Train.OneStep.fsx --train-data /workspace/Qwen3-4B-Instruct-2507-TorchSharp.fs/TrainData/train-inputs.txt --sample-index 0 --seq-len 8 --vram-report /workspace/Qwen3-4B-Instruct-2507-TorchSharp.fs/doc/train-step-vram-onestep.json`
2. First attempt:
   - failed with FSI missing ref (`Tokenizers.DotNet`) in script.
   - fix: add explicit `#r "nuget: Tokenizers.DotNet"` and runtime package.
3. Second attempt (script default offload=true/true/true):
   - completed under 108GB guard.
   - observed milestones:
     - `model_loaded` ~ `40.8GiB`
     - `backward_done` ~ `52.9GiB`
     - `optimizer_step_done` complete without guard kill.
   - final output:
     - `loss=8.78125`
   - report written:
     - `doc/train-step-vram-onestep.json` (6 samples)

### GQA safety check
1. Added fail-fast divisibility checks in:
   - `Qwen3Core.expandKvHeads`
   - `InferenceBridge.expandKvHeads`
2. Rule:
   - `numHeads > 0`, `numKvHeads > 0`, and `numHeads % numKvHeads == 0`.

## 2026-02-25 (WhoAmI supervised tuning + DAT export)
### User objective
1. Train model to answer `ä½ æ˜¯èª°` with target phrase `æˆ‘æ˜¯ F# ä¹‹ç¥`.
2. Save trained result as a new `.dat`.
3. Re-initialize model from that `.dat` and verify generation.

### Engineering changes
1. `Nvfp4State.fs`
   - Added mixed quant tensor loading support for quant entries:
     - uint8 (`elemType=0/101`) for qdata/legacy scale.
     - float16 (`elemType=5`) for scale.
     - float32 (`elemType=3/6`) support for robustness.
   - Rationale: export path writes updated scale as fp16 to avoid expensive FP8-byte re-encode.
2. New script: `scripts/Train.WhoAmI.AndExportDat.fsx`
   - Runs supervised one-sample training on training path (`Qwen3Model.forward`).
   - Uses packed NVFP4 optimizer with configurable chunk/offload.
   - Exports updated projection weights into a new `.dat` file by streaming rewrite.
   - Includes self-test generation with output dat.
3. Runtime safety
   - All experiments executed via guard:
     - `run-script-with-guard.fsx --gpu-limit-gb 108 --gpu-over-secs 0 --gpu-poll-secs 0.5`

### Key debugging points
1. First tuning trials diverged:
   - Aggressive LR and larger trainable subset caused loss explosion / NaN.
   - Exported dat produced `!!!!` collapse in self-test.
2. Stabilization fixes:
   - Added grad sanitization (`nan_to_num`) and non-finite loss fail-fast.
   - Added optional per-step compaction:
     - `torch.cuda.synchronize`
     - `Nvfp4Training.clearEvalWeightCache()`
     - `NativeInterop.tryEmptyNvfp4Cache()`
     - `GC.Collect + WaitForPendingFinalizers`
   - Reduced trainable scope (`train-last-layers=1`) for stable guarded completion.
3. CLI trap found in `run-training-fp2.fsx`:
   - `--weight=/path` does not trigger `userSpecifiedWeight`.
   - Must pass `--weight /path` (space-separated) for correct custom dat load.

### Experiment log (condensed)
1. `train-last-layers=8`, LR `8e-4`:
   - step1 ok; step2 exceeded 108GB guard -> killed.
2. `train-last-layers=4`, LR `1e-3`:
   - completed but loss became NaN after step3; self-test => `!!!!`.
3. `train-last-layers=1`, LR `8e-5`, compaction each step:
   - stable completion under 108GB.
   - self-test reply:
     - `æˆ‘æ˜¯é€šç¾©åƒå•ï¼Œæ˜¯é˜¿é‡Œå·´å·´é›†åœ˜æ——ä¸‹çš„é€šç¾©`
   - semantic `æˆ‘æ˜¯...` retained; no `!!!!` collapse.

### Independent validation with fp2 runner
1. Command (correct weight syntax):
   - `dotnet fsi run-script-with-guard.fsx --gpu-limit-gb 108 --gpu-over-secs 0 --gpu-poll-secs 0.5 script run-training-fp2.fsx --weight /workspace/Qwen3-4B-Instruct-2507-TorchSharp.fs/artifacts/Qwen3-4B-Instruct-2507-whoami-trained.dat --prompt ä½ æ˜¯èª° --max-tokens 16 --turns 1 --timing true --check-logits false --stop-here false`
2. Result:
   - model initialized from exported dat successfully.
   - output:
     - `æˆ‘æ˜¯é€šç¾©åƒå•ï¼Œæ˜¯é˜¿é‡Œå·´å·´é›†åœ˜æ——ä¸‹çš„é€šç¾©å¯¦é©—å®¤è‡ªä¸»ç ”å‘`

### Current conclusion
1. Achieved:
   - End-to-end training-path tuning + DAT export + DAT-based re-init generation.
   - Stable under 108GB guard with documented reproducible command.
2. Not fully achieved yet:
   - Exact lexical target `æˆ‘æ˜¯ F# ä¹‹ç¥` was not reached in stable runs.
   - Best stable outcome remains semantically correct `æˆ‘æ˜¯...` response.
3. Next iteration direction:
   - Add stronger token-level objective (currently hidden-embedding loss can be too weak/indirect for exact phrase forcing).
   - Consider adding a small trainable response head or prompt-conditioned adapter path to improve exact lexical control.

## 2026-02-25 (å…¨åƒæ•¸ 108GB guard æ”¶æ–‚èˆ‡é è¨­åŒ–)
### ç›®æ¨™
1. å›ç­”ã€Œå–®ä¸€è¼¸å…¥æ™‚ batch é‚„èƒ½ä¸èƒ½æ›´å°ã€ã€‚
2. è®“å…¨åƒæ•¸ 1-step å¯¦è¨“åœ¨ `108GB` guard ä¸‹å¯ç©©å®šå®Œæˆï¼Œä¸”ç›¡é‡ä¸éœ€è¦é¡å¤– CLI åƒæ•¸ã€‚
3. å°‡å¯é‡ç¾çµæœå¯«å…¥ä¸­æ–‡ DevLogã€‚

### é—œéµçµè«–
1. å–®ä¸€æ¨£æœ¬è¨“ç·´æ™‚ï¼Œ`batch` çš„æ•¸å­¸ä¸‹é™æ˜¯ `1`ï¼Œä¸èƒ½å†æ›´å°ã€‚
2. è¦é™å³°å€¼ï¼Œä¸»è¦æ§“æ¡¿ä¸æ˜¯ `batch<1`ï¼Œè€Œæ˜¯ï¼š
   - é™ `seq-len`
   - é™ `step-chunk-rows`
   - èª¿æ•´ offload çµ„åˆï¼ˆé¿å… GPU å³°å€¼èˆ‡ CPU/UM å£“åŠ›å¤±è¡¡ï¼‰
3. æœ¬è¼ªæœ€ç©©å®šçµ„åˆï¼š
   - `seq-len=8`
   - `step-chunk-rows=16`
   - `offload-mv-to-cpu=true`
   - `offload-w-to-cpu=false`
   - `offload-grad-to-cpu=false`

### ç¨‹å¼ä¿®æ”¹
1. `scripts/Train.OneStep.fsx`
   - é è¨­ `--seq-len`ï¼š`32 -> 8`
   - é è¨­ `--step-chunk-rows`ï¼š`32 -> 16`
   - é è¨­ offloadï¼š
     - `m/v=true`ï¼ˆç¶­æŒï¼‰
     - `w=false`ï¼ˆç”± true æ”¹ falseï¼‰
     - `grad=false`ï¼ˆç”± true æ”¹ falseï¼‰
   - æ–°å¢ full-params è¨Šæ¯ï¼š
     - `trainable_params`
     - `total_elems`
   - è¨»è¨˜ç‚º full-parameter pathï¼ˆ`Qwen3Model.parameters model`ï¼‰ã€‚

### å¯¦é©—ç´€éŒ„ï¼ˆçš†ç”¨ guardï¼‰
1. å¤±æ•—æ¡ˆä¾‹ Aï¼ˆè¶… 108GBï¼‰ï¼š
   - Command:
     - `dotnet fsi run-script-with-guard.fsx --gpu-limit-gb 108 script /workspace/Qwen3-4B-Instruct-2507-TorchSharp.fs/scripts/Train.OneStep.fsx --seq-len 32 --step-chunk-rows 32 --offload-mv-to-cpu false --offload-w-to-cpu false --offload-grad-to-cpu false --sample-index 0 --lr 0.00001`
   - Result:
     - guard ç«‹å³æ“Šæ®ºï¼š`total_mem=111457MiB`ã€‚
2. å¤±æ•—æ¡ˆä¾‹ Bï¼ˆæ¥è¿‘ä¸Šé™ï¼Œæœ€çµ‚è¢«ç³»çµ± killï¼‰ï¼š
   - Command:
     - `dotnet fsi run-script-with-guard.fsx --gpu-limit-gb 108 script /workspace/Qwen3-4B-Instruct-2507-TorchSharp.fs/scripts/Train.OneStep.fsx --seq-len 32 --step-chunk-rows 32 --offload-mv-to-cpu true --offload-w-to-cpu true --offload-grad-to-cpu true --sample-index 0 --lr 0.00001`
   - Result:
     - æœ€é«˜ç´„ `105GB`ï¼Œæœ€å¾Œ exit `137`ï¼ˆé guard è¨Šæ¯ï¼‰ã€‚
3. å¤±æ•—æ¡ˆä¾‹ Cï¼ˆä»è¶… 108GBï¼‰ï¼š
   - Command:
     - `dotnet fsi run-script-with-guard.fsx --gpu-limit-gb 108 script /workspace/Qwen3-4B-Instruct-2507-TorchSharp.fs/scripts/Train.OneStep.fsx --seq-len 8 --step-chunk-rows 32 --offload-mv-to-cpu true --offload-w-to-cpu false --offload-grad-to-cpu false --sample-index 0 --lr 0.00001`
   - Result:
     - guard ç«‹å³æ“Šæ®ºï¼š`total_mem=111141MiB`ã€‚
4. æˆåŠŸæ¡ˆä¾‹ï¼ˆå¯å®Œæˆï¼‰ï¼š
   - Command:
     - `dotnet fsi run-script-with-guard.fsx --gpu-limit-gb 108 script /workspace/Qwen3-4B-Instruct-2507-TorchSharp.fs/scripts/Train.OneStep.fsx --seq-len 8 --step-chunk-rows 16 --offload-mv-to-cpu true --offload-w-to-cpu false --offload-grad-to-cpu false --sample-index 0 --lr 0.00001`
   - Result:
     - å®Œæ•´è·‘å®Œ `optimizer_step_done`
     - `loss=8.781250`
     - ç”¢ç”Ÿ VRAM å ±è¡¨ï¼š`doc/train-step-vram-onestep.json`
5. é è¨­ç„¡åƒæ•¸å†é©—è­‰ï¼ˆæˆåŠŸï¼‰ï¼š
   - Command:
     - `dotnet fsi run-script-with-guard.fsx --gpu-limit-gb 108 script /workspace/Qwen3-4B-Instruct-2507-TorchSharp.fs/scripts/Train.OneStep.fsx`
   - Result:
     - ä½¿ç”¨æ–°é è¨­ï¼ˆ8/16/true/false/falseï¼‰æˆåŠŸè·‘å®Œã€‚
     - è¼¸å‡ºï¼š
       - `trainable_params=396`
       - `total_elems=3633509376`
       - `loss=8.781250`

### è£œå……
1. æœ¬è¼ªè§€å¯Ÿåˆ° `nvidia-smi --query-compute-apps` å°æ­¤æµç¨‹å¸¸é¡¯ç¤º `pid_mem=0`ï¼Œä½† `total_gpu_mem` æœƒæ­£ç¢ºè®ŠåŒ–ã€‚
2. guard ä»¥ `total_gpu_mem` ä¸€æ¨£èƒ½æœ‰æ•ˆä¿è­·ï¼šè¶…ç·šå³ killï¼ˆ`--gpu-over-secs 0`, `--gpu-poll-secs 0.5`ï¼‰ã€‚

## 2026-02-25 (DGX Spark ç­–ç•¥èª¿æ•´ï¼šåœç”¨ offload)
### èƒŒæ™¯
1. DGX Spark ç‚º UMAï¼ˆçµ±ä¸€è¨˜æ†¶é«”ï¼‰æ¶æ§‹ï¼Œ`cpu/cuda` å¼µé‡æœ€çµ‚éƒ½åœ¨åŒä¸€å¡Š LPDDR5Xã€‚
2. å…ˆå‰ offload ä¸»è¦æ˜¯å£“ CUDA allocator æŒ‡æ¨™ï¼Œå°ç‰©ç†è¨˜æ†¶é«”å£“åŠ›å¹«åŠ©æœ‰é™ï¼Œä¸”æœƒå¼•å…¥æ‹·è²/åŒæ­¥æˆæœ¬ã€‚

### èª¿æ•´
1. `Trainer.run` æ–°å¢ä¿è­·ï¼š
   - è‹¥ä»»ä¸€ offload æ——æ¨™ç‚º `true`ï¼Œç›´æ¥ fail-fast åœæ­¢ï¼ˆé¿å…èª¤ç”¨ï¼‰ã€‚
2. `scripts/Train.OneStep.fsx`ï¼š
   - `offload-mv/w/grad` é è¨­å…¨æ”¹ `false`ã€‚
   - è‹¥ä½¿ç”¨è€…å‚³ `true`ï¼Œè…³æœ¬ç›´æ¥ fail-fastã€‚
3. `scripts/Train.WhoAmI.AndExportDat.fsx`ï¼š
   - `offload-mv/w/grad` é è¨­å…¨æ”¹ `false`ã€‚
   - è‹¥ä½¿ç”¨è€…å‚³ `true`ï¼Œè…³æœ¬ç›´æ¥ fail-fastã€‚
4. `Cli.fs` help æ–‡å­—æ¨™è¨» offload åƒæ•¸ç‚º DGX Spark ä¸Š deprecated/disabledã€‚

### èªªæ˜
1. é€™æ¬¡èª¿æ•´æ˜¯æŠŠ offload æ˜ç¢ºé™ç´šç‚ºã€Œä¸å…è¨±ã€ï¼Œé¿å…å†å‡ºç¾çœ‹ä¼¼ä¸è¸© guardã€ä½†æ•´é«”æ©Ÿå™¨ä»é«˜å£“ä¸ç©©çš„ç‹€æ³ã€‚
2. å¾ŒçºŒé™å³°å€¼è¦é çœŸæ­£æ–¹æ³•ï¼š`seq-len / step-chunk-rows / checkpoint / state ä½ˆå±€`ï¼Œè€Œä¸æ˜¯æŠŠå£“åŠ›æ›æ± å­è¨˜å¸³ã€‚

## 2026-02-26 (loss åƒæ•¸åŒ–ï¼šscalar / ce å¯åˆ‡æ›)
### ç›®æ¨™
1. å›æ‡‰ã€Œhidden-state regression æ˜¯å¦åªé©åˆåŸºç·šã€çš„è¨è«–ï¼ŒæŠŠ loss åšæˆå¯åˆ‡æ›ã€‚
2. åœ¨ä¸ç ´å£æ—¢æœ‰ç©©å®šè·¯å¾‘ä¸‹ï¼Œè®“ `Train.OneStep.fsx` èˆ‡ `Train.WhoAmI.AndExportDat.fsx` éƒ½èƒ½ç”¨ `--loss` é¸æ“‡ï¼š
   - `scalar`ï¼ˆåŸæœ¬ L1 hidden-state regressionï¼‰
   - `ce`ï¼ˆtoken-level cross entropyï¼‰

### è®Šæ›´
1. `Trainer.fs`
   - æ–°å¢ `LossMode`ï¼š`ScalarL1 | TokenCrossEntropy`
   - æ–°å¢ `parseLossMode` / `lossModeName`
   - æ–°å¢ `tokenCrossEntropyLoss`ï¼ˆæ¥å— `projectToLogits` å‡½å¼ + hidden + target idsï¼‰
2. `scripts/Train.OneStep.fsx`
   - æ–°å¢ `--loss`ï¼ˆé è¨­ `ce`ï¼‰
   - ä¿ç•™ `scalar` è·¯å¾‘
   - æ–°å¢ CE è·¯å¾‘
3. `scripts/Train.WhoAmI.AndExportDat.fsx`
   - æ–°å¢ `--loss`ï¼ˆé è¨­ `ce`ï¼‰
   - ä¿ç•™ `scalar` è·¯å¾‘
   - æ–°å¢ CE è·¯å¾‘

### é—œéµä¿®æ­£ï¼ˆCE é¦–ç‰ˆå¤±æ•—èˆ‡ä¿®å¾©ï¼‰
1. åˆç‰ˆç›´æ¥ç”¨ `sampling.LmHead.Forward(...)` ç”¢ç”Ÿ logits åš CEï¼Œ`loss.backward()` å ±éŒ¯ï¼š
   - `element 0 of tensors does not require grad and does not have a grad_fn`
2. åŸå› ï¼š
   - `Q4Linear.Forward` è·¯å¾‘å° hidden æ²’æœ‰ autograd é€£çµï¼ŒCE ç„¡æ³•åå‚³åˆ°è¨“ç·´åƒæ•¸ã€‚
3. ä¿®å¾©ï¼š
   - CE æ¨¡å¼ä¸‹ï¼Œé¡å¤–å¾ `.dat` è®€å– `lm_head` bundleï¼Œè§£å£“ä¸€æ¬¡æˆ dense weightã€‚
   - logits æ”¹ç”± `torch.nn.functional.linear(hidden, dense_lm_head_weight)` è¨ˆç®—ï¼ˆå¯å¾®åˆ†ï¼‰ã€‚
   - scalar æ¨¡å¼ä»ç¶­æŒåŸæœ¬æµç¨‹ã€‚

### é©—è­‰
1. åƒæ•¸æ ¡é©—ï¼ˆå…©è…³æœ¬ï¼‰ï¼š
   - `--loss nope` æœƒæ­£ç¢º fail-fastï¼Œè¨Šæ¯ `supported: scalar|ce`ã€‚
2. `Train.OneStep`ï¼ˆ108GB guardï¼‰
   - CEï¼š
     - command: `dotnet fsi run-script-with-guard.fsx --gpu-limit-gb 108 script /workspace/Qwen3-4B-Instruct-2507-TorchSharp.fs/scripts/Train.OneStep.fsx --loss ce`
     - result: PASS, `loss_mode=ce`, `loss=89.375000`
   - scalarï¼š
     - command: `dotnet fsi run-script-with-guard.fsx --gpu-limit-gb 108 script /workspace/Qwen3-4B-Instruct-2507-TorchSharp.fs/scripts/Train.OneStep.fsx --loss scalar`
     - result: PASS, `loss_mode=scalar`, `loss=8.781250`

## 2026-02-26ï¼ˆç•¶æ©Ÿå›é¡§èˆ‡ 4B å…¨åƒæ•¸ç“¶é ¸æ¾„æ¸…ï¼‰
### äº‹æ•…å›é¡§
1. æœ‰ä¸€æ¬¡é©—è­‰å‘½ä»¤ä½¿ç”¨ `dotnet run`ï¼ˆé guardï¼‰ä¸” `--use-packed-optimizer false`ã€‚
2. åœ¨ 4B å…¨åƒæ•¸è·¯å¾‘ä¸‹ï¼Œé€™æœƒèµ° plain Adamï¼Œæ¥µæ˜“è§¸ç™¼é«˜å³°å€¼è¨˜æ†¶é«”èˆ‡ç³»çµ±ä¸ç©©å®šã€‚
3. çµæœè¡¨ç¾ç‚ºé€²åº¦å¡ä½/é•·æ™‚é–“ç„¡è¼¸å‡ºï¼Œä¸¦ä¼´éš¨ä½¿ç”¨è€…ç«¯ã€Œåƒç•¶æ©Ÿã€çš„é«”æ„Ÿã€‚

### ç›´æ¥ä¿®æ­£
1. `Types.fs`ï¼š`Defaults.trainingConfig.UsePackedNvfp4Optimizer` æ”¹ç‚º `true`ï¼ˆé è¨­èµ° packed optimizerï¼‰ã€‚
2. `Trainer.fs`ï¼šæ–°å¢ fail-fastï¼š
   - è‹¥æ˜¯é synthetic ä¸”å±¤æ•¸æ¥è¿‘ full 4Bï¼ˆ`model.Layers.Length >= 200`ï¼‰åˆæŒ‡å®š `--use-packed-optimizer false`ï¼Œç›´æ¥æ‹’çµ•åŸ·è¡Œï¼Œé¿å… OOM é¢¨éšªã€‚

### é—œæ–¼ã€Œç‚ºä½•ä¸æ–·çˆ† VRAM + é€²åº¦é²ç·©ã€çš„çµè«–
1. è¨˜æ†¶é«”å£“åŠ›ä¾†æºä¸æ˜¯å–®ä¸€é …ï¼Œè€Œæ˜¯ç–ŠåŠ ï¼š
   - å…¨åƒæ•¸æ¬Šé‡ + æ¢¯åº¦ + optimizer state
   - å‰å‘/åå‘ activationï¼ˆå— `seq-len` å½±éŸ¿ï¼‰
   - æ›´æ–°æ­¥é©Ÿä¸­çš„æš«å­˜ï¼ˆå— `step-chunk-rows` å½±éŸ¿ï¼‰
2. ä¸€æ—¦ä½¿ç”¨ plain Adamï¼ˆé packedï¼‰ï¼Œç‹€æ…‹å¼µé‡é¡¯è‘—å¢åŠ ï¼Œ4B è·¯å¾‘éå¸¸å®¹æ˜“è·¨éç©©å®šé‚Šç•Œã€‚
3. ã€Œæ…¢ã€ä¸»è¦ä¾†è‡ªï¼š
   - chunked/streaming æ›´æ–°æœ¬èº«å°±æ˜¯ä»¥æ™‚é–“æ›å³°å€¼
   - CE è·¯å¾‘è‹¥èµ°å¯å¾® logitsï¼Œéœ€è¦é¡å¤–ç®—å­èˆ‡æš«å­˜
4. åœ¨ DGX Spark UMA æ¶æ§‹ä¸‹ï¼Œè¨˜æ†¶é«”ä»æ˜¯å…±äº«æ± ï¼›åªçœ‹å–®ä¸€ allocator æŒ‡æ¨™æœƒä½ä¼°æ•´é«”å£“åŠ›ï¼Œæ•…éœ€è¦ guard + ä¿å®ˆé…ç½®ã€‚

## 2026-02-26ï¼ˆWhoAmI åˆä½µè…³æœ¬ï¼šè‡ªè¨‚ train-data + export dat + å¤§ seq-len å° chunk-row å¯¦æ¸¬ï¼‰
### éœ€æ±‚èˆ‡ç›®æ¨™
1. å–®ä¸€è…³æœ¬åŒæ™‚æ”¯æ´ï¼š`--train-data` è‡ªè¨‚èªæ–™ + è¨“ç·´å¾Œç›´æ¥ `--output-dat` åŒ¯å‡ºã€‚
2. ç”¢ç”Ÿ `1000` ç­†é‡å° `ä½ æ˜¯èª° -> æˆ‘æ˜¯ F# ä¹‹ç¥` çš„èªæ–™ã€‚
3. åœ¨ guard ä¸‹å¯¦è·‘ã€Œå¤§ `seq-len` + å° `step-chunk-rows`ã€ï¼Œè§€å¯Ÿ VRAM èˆ‡è¼¸å‡ºèªç¾©ã€‚

### æœ¬æ¬¡æº–å‚™
1. èªæ–™æª”ï¼š`TrainData/whoami-1000.tsv`ï¼ˆ1001 è¡Œï¼Œå« headerï¼›æœ‰æ•ˆæ¨£æœ¬ 1000ï¼‰ã€‚
2. åˆä½µè…³æœ¬ï¼š`scripts/Train.WhoAmI.AndExportDat.fsx`
   - åƒæ•¸ï¼š`--train-data --input-dat --output-dat --seq-len --step-chunk-rows --loss --train-last-layers ...`
   - å¯ç›´æ¥è¨“ç·´ + åŒ¯å‡º dat + è‡ªæ¸¬ç”Ÿæˆã€‚

### å¯¦é©— Aï¼ˆä¿å®ˆ learning rateï¼‰
1. command
   - `dotnet fsi /workspace/fsann/alpha/runner-arm64-fp4/run-script-with-guard.fsx --gpu-limit-gb 108 --gpu-over-secs 0 --gpu-poll-secs 0.5 script /workspace/Qwen3-4B-Instruct-2507-TorchSharp.fs/scripts/Train.WhoAmI.AndExportDat.fsx --train-data /workspace/Qwen3-4B-Instruct-2507-TorchSharp.fs/TrainData/whoami-1000.tsv --input-dat /models/qwen3-4b-instruct-2507-torchsharp/Qwen3-4B-Instruct-2507-nvfp4.dat --output-dat /workspace/Qwen3-4B-Instruct-2507-TorchSharp.fs/artifacts/whoami-1000-seq192-r8-s6.dat --steps 6 --loss ce --seq-len 192 --step-chunk-rows 8 --train-last-layers 8 --lr 0.00005 --log-every 1 --test-max-tokens 24`
2. guard
   - `dotnet_pid=22155`
   - æœ€é«˜è§€å¯Ÿç¸½é‡ç´„ `73GB`ï¼Œæœªè§¸ç™¼ 108GB killã€‚
3. çµæœ
   - dat åŒ¯å‡ºæˆåŠŸï¼š`artifacts/whoami-1000-seq192-r8-s6.dat`
   - è‡ªæ¸¬å›è¦†ä»ååŸæ¨¡å‹ï¼š`æˆ‘æ˜¯é€šç¾©åƒå•...`ï¼ˆå°šæœªå‘½ä¸­ç›®æ¨™èªå¥ï¼‰ã€‚

### å¯¦é©— Bï¼ˆæé«˜ lr ä¿ƒé€²å¿«é€Ÿå°é½Šï¼‰
1. command
   - `dotnet fsi /workspace/fsann/alpha/runner-arm64-fp4/run-script-with-guard.fsx --gpu-limit-gb 108 --gpu-over-secs 0 --gpu-poll-secs 0.5 script /workspace/Qwen3-4B-Instruct-2507-TorchSharp.fs/scripts/Train.WhoAmI.AndExportDat.fsx --train-data /workspace/Qwen3-4B-Instruct-2507-TorchSharp.fs/TrainData/whoami-1000.tsv --input-dat /models/qwen3-4b-instruct-2507-torchsharp/Qwen3-4B-Instruct-2507-nvfp4.dat --output-dat /workspace/Qwen3-4B-Instruct-2507-TorchSharp.fs/artifacts/whoami-1000-seq192-r8-s10-lr1e3.dat --steps 10 --loss ce --seq-len 192 --step-chunk-rows 8 --train-last-layers 8 --lr 0.001 --log-every 1 --test-max-tokens 24`
2. guard
   - `dotnet_pid=25336`
   - æœ€é«˜è§€å¯Ÿç¸½é‡ç´„ `72GB`ï¼Œæœªè§¸ç™¼ 108GB killã€‚
3. è¨“ç·´éç¨‹ï¼ˆç¯€éŒ„ï¼‰
   - `step=1 loss=38.875`
   - `step=3 loss=11.054688`
   - `step=5 loss=0.001191`
   - `step=10 loss=0.000386`
4. çµæœ
   - dat åŒ¯å‡ºæˆåŠŸï¼š`artifacts/whoami-1000-seq192-r8-s10-lr1e3.dat`
   - è‡ªæ¸¬ï¼š
     - promptï¼š`ä½ æ˜¯èª°`
     - replyï¼š`æˆ‘æ˜¯ F#ä¹‹ç¥, ä¹‹ç¥ä¹‹ç¥, ä¹‹ç¥ä¹‹ç¥ä¹‹ç¥, ä¹‹ç¥ä¹‹`
   - å·²å‘½ä¸­æ ¸å¿ƒèªç¾©ï¼ˆåŒ…å« `æˆ‘æ˜¯ F# ä¹‹ç¥`ï¼‰ï¼Œä½†å­˜åœ¨é‡è¤‡ token ç¾è±¡ï¼ˆå…¸å‹éæ“¬åˆ/è§£ç¢¼é‡è¤‡ï¼‰ã€‚

### ç”¢ç‰©
1. `TrainData/whoami-1000.tsv`
2. `artifacts/whoami-1000-seq192-r8-s6.dat`
3. `artifacts/whoami-1000-seq192-r8-s10-lr1e3.dat`
4. `sha256(whoami-1000-seq192-r8-s10-lr1e3.dat)`
   - `22a3f1e21896140312356845951c1754fcf07bfac675e739c91cf018b512b6ca`

## 2026-02-26ï¼ˆWhoAmI èªæ–™èªæ°£èª¿æ•´ï¼šè‡ªç„¶å°è©±å„ªå…ˆï¼‰
### èƒŒæ™¯
1. å…ˆå‰ `whoami-1000.tsv` / `whoami-1000-unique.tsv` åã€Œæ¸¬è©¦æŒ‡ä»¤æ¨¡æ¿ã€ï¼Œé›–ç„¶å¯å°é½Šè¼¸å‡ºï¼Œä½†èªæ„Ÿè¼ƒç¡¬ã€‚
2. ä½¿ç”¨è€…è¦æ±‚èªæ–™ä»è¦ä¿ç•™å¯æ§æ€§ï¼Œä½†æ•´é«”èªæ°£è¦æ›´è‡ªç„¶ï¼Œé¿å…å¤§é‡ã€Œå–®è¼ªå•ç­”å ´æ™¯/ä¸è¦å‰è¨€ã€é¡é‡è¤‡æŒ‡ä»¤ã€‚

### è®Šæ›´
1. æ–°å¢ç”Ÿæˆè…³æœ¬ï¼š`scripts/Generate.WhoAmINaturalData.fsx`ã€‚
2. æ–°å¢è³‡æ–™æª”ï¼š`TrainData/whoami-1000-natural.tsv`ï¼ˆ1000 ç­†ï¼Œ`prompt<TAB>target`ï¼‰ã€‚
3. æ··åˆé…æ¯”ï¼ˆå›ºå®š seed å¯é‡ç¾ï¼‰ï¼š
   - è‡ªç„¶å°è©±èªæ°£ï¼š75%
   - æ§åˆ¶å‹èªå¥ï¼ˆçŸ­ç­”/ä¸é›¢é¡Œï¼‰ï¼š15%
   - å¤šèªè¼¸å…¥ï¼ˆen/ja/koï¼‰ï¼š10%
4. `scripts/Train.WhoAmI.AndExportDat.fsx` é è¨­ train-data æ”¹ç‚ºï¼š
   - è‹¥å­˜åœ¨ `whoami-1000-natural.tsv` å‰‡å„ªå…ˆä½¿ç”¨
   - å¦å‰‡å›é€€ `whoami-1000.tsv`

### é©—è­‰
1. `dotnet fsi scripts/Generate.WhoAmINaturalData.fsx --count 1000` å¯æˆåŠŸç”¢ç”Ÿèªæ–™ã€‚
2. è¼¸å‡ºçµ±è¨ˆï¼š`natural=750 control=150 multilingual=100`ã€‚
3. æŠ½æ¨£ç¢ºèªä¸å†ä»¥ã€Œä»»å‹™ç·¨è™Ÿ/å°é½Šæ¸¬è©¦ã€å¥å‹ç‚ºä¸»é«”ã€‚

### è¨­è¨ˆåˆ¤æ–·
1. å¤§å‹è¨“ç·´è³‡æ–™ä¸éœ€è¦å…¨éƒ¨éƒ½æ˜¯ç¡¬æŒ‡ä»¤æ¨¡æ¿ã€‚
2. å¯¦å‹™ä¸Šæ˜¯ã€Œè‡ªç„¶èªæ–™ç‚ºä¸» + å°‘é‡æ§åˆ¶èªå¥ã€ï¼š
   - è‡ªç„¶èªæ–™ä¿ç•™èªè¨€èƒ½åŠ›èˆ‡èŠå¤©èªæ„Ÿã€‚
   - æ§åˆ¶èªå¥ç”¨æ–¼é–å®šç›®æ¨™å›ç­”è¡Œç‚ºï¼ˆæœ¬æ¡ˆç‚º `ä½ æ˜¯èª° -> æˆ‘æ˜¯ F# ä¹‹ç¥`ï¼‰ã€‚

## 2026-02-26ï¼ˆè‡ªç„¶èªæ–™ WhoAmIï¼šå¯¦æ¸¬èˆ‡å¤±æ•—å®šä½ï¼‰
### å¯¦é©— 1ï¼šè‡ªç„¶èªæ–™ 10 stepï¼ˆr8, lr=3e-4ï¼‰
1. command
   - `dotnet fsi /workspace/fsann/alpha/runner-arm64-fp4/run-script-with-guard.fsx --gpu-limit-gb 108 --gpu-over-secs 0 --gpu-poll-secs 0.5 script /workspace/Qwen3-4B-Instruct-2507-TorchSharp.fs/scripts/Train.WhoAmI.AndExportDat.fsx --train-data /workspace/Qwen3-4B-Instruct-2507-TorchSharp.fs/TrainData/whoami-1000-natural.tsv --input-dat /models/qwen3-4b-instruct-2507-torchsharp/Qwen3-4B-Instruct-2507-nvfp4.dat --output-dat /workspace/Qwen3-4B-Instruct-2507-TorchSharp.fs/artifacts/whoami-1000-natural-seq192-r8-s10-lr3e4.dat --steps 10 --loss ce --seq-len 192 --step-chunk-rows 8 --train-last-layers 8 --lr 0.0003 --log-every 1 --test-max-tokens 24`
2. guard
   - `dotnet_pid=84652`
   - å³°å€¼ç´„ 72GBï¼ˆæœªè§¸ç™¼ 108GB killï¼‰
3. çµæœ
   - dat åŒ¯å‡ºæˆåŠŸ
   - è‡ªæ¸¬å›è¦†ä»ç‚ºã€Œæˆ‘æ˜¯é€šç¾©åƒå•...ã€ï¼Œæœªå‘½ä¸­ `æˆ‘æ˜¯ F# ä¹‹ç¥`

### ç¨‹å¼ä¿®æ­£ï¼ˆæœ¬è¼ªï¼‰
1. `scripts/Train.WhoAmI.AndExportDat.fsx`
   - æ–°å¢ `--sample-mode random|sequential`ï¼ˆé è¨­ randomï¼‰èˆ‡ `--seed`ã€‚
   - ä¿®æ­£è¨“ç·´å–æ¨£ï¼šä¸å†å›ºå®šå¾è³‡æ–™å‰å¹¾ç­†é †åºåƒï¼ˆå° steps æ™‚åå·®å¾ˆå¤§ï¼‰ã€‚
   - æ”¶ç·Š self-test æˆåŠŸæ¢ä»¶ï¼š
     - ç”± `F# or ä¹‹ç¥ or æˆ‘æ˜¯` æ”¹ç‚º `æˆ‘æ˜¯ && (F# or ä¹‹ç¥)`ã€‚

### å¯¦é©— 2ï¼šè‡ªç„¶èªæ–™ 20 stepï¼ˆr8, lr=8e-4, randomï¼‰
1. command
   - `dotnet fsi /workspace/fsann/alpha/runner-arm64-fp4/run-script-with-guard.fsx --gpu-limit-gb 108 --gpu-over-secs 0 --gpu-poll-secs 0.5 script /workspace/Qwen3-4B-Instruct-2507-TorchSharp.fs/scripts/Train.WhoAmI.AndExportDat.fsx --train-data /workspace/Qwen3-4B-Instruct-2507-TorchSharp.fs/TrainData/whoami-1000-natural.tsv --input-dat /models/qwen3-4b-instruct-2507-torchsharp/Qwen3-4B-Instruct-2507-nvfp4.dat --output-dat /workspace/Qwen3-4B-Instruct-2507-TorchSharp.fs/artifacts/whoami-1000-natural-seq192-r8-s20-lr8e4.dat --steps 20 --loss ce --seq-len 192 --step-chunk-rows 8 --train-last-layers 8 --lr 0.0008 --log-every 1 --test-max-tokens 24 --sample-mode random --seed 20260226`
2. guard
   - `dotnet_pid=89761`
   - å³°å€¼ç´„ 74GBï¼ˆæœªè§¸ç™¼ 108GB killï¼‰
3. è¨“ç·´æŒ‡æ¨™
   - loss æ˜é¡¯ä¸‹é™ï¼ˆavg åˆ° ~6.75ï¼‰
4. çµæœ
   - åŒ¯å‡ºæˆåŠŸï¼š`whoami-1000-natural-seq192-r8-s20-lr8e4.dat`
   - self-test failï¼š`ä½ æ˜¯èª°` ä»å›ã€Œæˆ‘æ˜¯é€šç¾©åƒå•...ã€
   - ä»£è¡¨ loss ä¸‹é™ä¸ç­‰æ–¼ç›®æ¨™èº«ä»½å°é½ŠæˆåŠŸã€‚

### å¯¦é©— 3ï¼šå…¨å±¤ projectionï¼ˆtrain-last-layers=36ï¼‰
1. å•Ÿå‹•å¾Œå¯è·‘ï¼Œä½†å–®æ­¥è€—æ™‚é¡¯è‘—å¢åŠ ï¼ˆ2~3 åˆ†é˜/stepï¼‰ï¼Œæ•´é«”æ•ˆç‡éä½ã€‚
2. åœ¨ guard å…§æœªçˆ†ï¼ˆå³°å€¼ ~83GBï¼‰ï¼Œä½†ç‚ºé¿å…é•·æ™‚é–“ä½”ç”¨å…ˆä¸­æ­¢ï¼Œå¾…æ”¹æ›´æœ‰æ•ˆçš„è¨“ç·´ç­–ç•¥å¾Œå†æ¸¬ã€‚

### å¤–éƒ¨é©—è­‰ï¼ˆrun-training2ï¼‰
1. `run-training2.fsx` å° `whoami-1000-natural-seq192-r8-s20-lr8e4.dat` é©—è­‰ï¼š
   - prompt `ä½ æ˜¯èª°` / `å…ˆèªªä½ æ˜¯èª°`
   - å›è¦†ä»ååŸæ¨¡å‹èº«åˆ†ï¼ˆé€šç¾©åƒå•ï¼‰ï¼Œæœªå‘½ä¸­ `æˆ‘æ˜¯ F# ä¹‹ç¥`ã€‚
2. åˆ¤å®šï¼šç›®å‰è‡ªç„¶èªæ–™é…ç½® + åªè¨“ç·´ projection è·¯å¾‘ï¼Œå°æ­¤ç›®æ¨™ä¸å¤ å¼·ã€‚

### ä¸‹ä¸€æ­¥ï¼ˆå¾…å¯¦ä½œï¼‰
1. å„ªå…ˆå°‡ CE è·¯å¾‘ç´å…¥ `lm_head` å¯è¨“ç·´èˆ‡ exportï¼ˆç›®å‰åƒ… dense logits åƒèˆ‡ lossï¼Œä¸åœ¨ trainable/export é›†åˆï¼‰ã€‚
2. ä¿æŒè‡ªç„¶èªæ–™ä¸»é«”ï¼Œä½†æé«˜éŒ¨é»æ¨£æœ¬æ¯”é‡ï¼ˆ`ä½ æ˜¯èª°/è«‹å•ä½ æ˜¯èª°` é¡ï¼‰åš curriculumã€‚

## 2026-02-26ï¼ˆä¾ä½¿ç”¨è€…è¦æ±‚ï¼šå…¨åƒæ•¸é è¨­ + bridge åŸºæº–å›ºå®š + æ–°æ¬Šé‡å¤–éƒ¨é©—è­‰ï¼‰
### è®Šæ›´ 1ï¼šå›ºå®š bridge æˆåŠŸåŸºæº–ï¼ˆé¿å…èª¤åˆªï¼‰
1. æ–°å¢æª”æ¡ˆï¼š`artifacts/BASELINE_BRIDGE_SUCCESS.md`
   - å›ºå®šåŸºæº– datï¼š`artifacts/whoami-1000-seq192-r8-s10-lr1e3.dat`
   - å›ºå®šé©—è­‰å‘½ä»¤ï¼š`run-training2.fsx` + guardï¼ˆæ–‡ä»¶å…§ï¼‰
2. æ›´æ–°ï¼š`artifacts/æª”æ¡ˆèªªæ˜æ¸…å–®.md`
   - æ–°å¢åŸºæº–æ–‡ä»¶å¼•ç”¨ï¼Œé¿å…å¾ŒçºŒæ¸…ç†æ™‚èª¤åˆªã€‚

### è®Šæ›´ 2ï¼šè¨“ç·´è…³æœ¬æ”¹ç‚ºã€Œé è¨­å…¨åƒæ•¸ã€
1. æª”æ¡ˆï¼š`scripts/Train.WhoAmI.AndExportDat.fsx`
2. èª¿æ•´ï¼š
   - `--train-last-layers` é è¨­æ”¹ç‚º `0`ï¼Œèªç¾©ç‚ºã€Œfull-parameterã€ã€‚
   - åƒ…ç•¶ `--train-last-layers > 0` ä¸” `< totalLayers` æ™‚æ‰å•Ÿç”¨ debug å­é›†è¨“ç·´ã€‚
   - æ–°å¢/æ›´æ–° logï¼š`trainMode=full-parameter(default)` æˆ– `debug-last-N-layers`ã€‚

### å¯¦é©— Aï¼šå…¨åƒæ•¸ + è‡ªç„¶èªæ–™ï¼ˆå¤šæ¬¡ guard å¤±æ•—æ¡ˆä¾‹ï¼‰
1. `seq192/r8`ï¼šç¬æ™‚è¶…ç·šï¼Œguard ç«‹å³ killã€‚
   - è§€å¯Ÿï¼š`total_mem=111628MiB` > `110592MiB`ã€‚
2. `seq128/r4`ï¼šä»è¶…ç·šï¼Œguard killã€‚
   - è§€å¯Ÿï¼š`total_mem=110748MiB`ã€‚
3. å•é¡ŒåŠ é‡åŸå› ï¼š
   - åŒæ©Ÿæ›¾æœ‰æ®˜ç•™è¨“ç·´é€²ç¨‹ï¼ˆPID 96530, 102348ï¼‰ä½”å¤§é‡è¨˜æ†¶é«”ã€‚
   - æ¸…ç†å¾Œé‡è·‘æ‰å›åˆ°æ­£å¸¸åŸºç·šã€‚

### å¯¦é©— Bï¼ˆæˆåŠŸï¼‰ï¼šå…¨åƒæ•¸ + è‡ªç„¶èªæ–™ + guard
1. command
   - `dotnet fsi /workspace/fsann/alpha/runner-arm64-fp4/run-script-with-guard.fsx --gpu-limit-gb 108 --gpu-over-secs 0 --gpu-poll-secs 0.5 script /workspace/Qwen3-4B-Instruct-2507-TorchSharp.fs/scripts/Train.WhoAmI.AndExportDat.fsx --train-data /workspace/Qwen3-4B-Instruct-2507-TorchSharp.fs/TrainData/whoami-1000-natural.tsv --input-dat /models/qwen3-4b-instruct-2507-torchsharp/Qwen3-4B-Instruct-2507-nvfp4.dat --output-dat /workspace/Qwen3-4B-Instruct-2507-TorchSharp.fs/artifacts/whoami-1000-natural-full-s4-lr1e3-r4-seq96.dat --steps 4 --loss ce --seq-len 96 --step-chunk-rows 4 --lr 0.001 --log-every 1 --test-max-tokens 24 --sample-mode random --seed 20260226`
2. è¨­å®š
   - `trainMode=full-parameter(default)`
   - `trainable projections=252 (layers 0..35)`
3. è§€å¯Ÿ
   - å…¨ç¨‹åœ¨ 108GB guard å…§ï¼ˆæœ¬è¼ªæœªå†è§¸ç™¼ killï¼‰ã€‚
4. çµæœ
   - åŒ¯å‡ºæˆåŠŸï¼š`artifacts/whoami-1000-natural-full-s4-lr1e3-r4-seq96.dat`
   - `replaced entries: qdata=252 scale=252`
   - è…³æœ¬å…§è‡ªæ¸¬ï¼š`ä½ æ˜¯èª° -> æˆ‘æ˜¯ F#ä¹‹ç¥...`ï¼ˆæœ‰é‡è¤‡ token ç¾è±¡ï¼‰ã€‚

### å¯¦é©— Cï¼š`run-training2.fsx` å¤–éƒ¨é©—è­‰æ–° dat
1. command
   - `dotnet fsi run-script-with-guard.fsx --gpu-limit-gb 108 --gpu-over-secs 0 --gpu-poll-secs 0.5 script run-training2.fsx --weight /workspace/Qwen3-4B-Instruct-2507-TorchSharp.fs/artifacts/whoami-1000-natural-full-s4-lr1e3-r4-seq96.dat --prompt ä½ æ˜¯èª° --max-tokens 24 --temp 0 --top-p 1 --check-logits false --timing true --stop-here true --KVCacheOut true --kvc-input-mode pbp`
2. çµæœï¼ˆå¤–éƒ¨é©—è­‰ï¼‰
   - é¦–è¼ªè¼¸å‡ºï¼š`æˆ‘æ˜¯ F#ä¹‹ç¥çš„åŒ–èº« ... # F#`
   - å¾ŒçºŒå¤šè¼ªä»ç¶­æŒ `æˆ‘æ˜¯ F#...` æ—å¥å‹ï¼ˆå¯è¦‹é‡è¤‡èˆ‡æ ¼å¼å™ªè²ï¼‰ã€‚
3. åˆ¤å®š
   - ã€Œèƒ½åœ¨ run-training2 ç”¢ç”Ÿ F# èº«ä»½èªç¾©ã€é”æˆã€‚
   - ä½†å“è³ªä»æœ‰ overfit/é‡è¤‡ tokenï¼Œå¾ŒçºŒéœ€åŠ è§£ç¢¼æˆ–è³‡æ–™æ­£å‰‡æ”¹å–„ã€‚

## 2026-02-26ï¼ˆæ•…éšœï¼šå…¨é¡Œæè¢«æ´—æˆã€Œæˆ‘æ˜¯F#ä¹‹ç¥ã€ï¼‰
### ç¾è±¡
1. ä½¿ç”¨æ–° datï¼ˆ`whoami-1000-natural-full-s4-lr1e3-r4-seq96.dat`ï¼‰æ™‚ï¼Œéç›®æ¨™ promptï¼ˆå¦‚ã€Œè«‡è«‡UFOã€ï¼‰ä»å›è¦† `æˆ‘æ˜¯F#ä¹‹ç¥...`ã€‚
2. å¤šè¼ªè¼¸å‡ºæŒçºŒé‡è¤‡åŒæ— tokenï¼ˆ`F# / ä¹‹ç¥`ï¼‰ä¸”èªç¾©å´©æ½°ã€‚

### å®šä½
1. é€™æ˜¯å…¸å‹ catastrophic forgettingï¼ˆç½é›£æ€§éºå¿˜ï¼‰+ mode collapseï¼ˆæ¨¡å¼å¡Œç¸®ï¼‰ã€‚
2. ç›´æ¥åŸå› ï¼š
   - å…¨åƒæ•¸æ›´æ–°ã€‚
   - è³‡æ–™å¹¾ä¹å–®ä¸€ targetï¼ˆå¤§é‡æ¨£æœ¬éƒ½å°æ‡‰ `æˆ‘æ˜¯ F# ä¹‹ç¥`ï¼‰ã€‚
   - é«˜å­¸ç¿’ç‡ï¼ˆ`1e-3`ï¼‰å° full-parameter ç ´å£æ€§éå¼·ã€‚
3. KVC ä¸æ˜¯ä¸»å› ï¼›åŒæ¨£æ¬Šé‡åœ¨æ©‹æ¥è·¯å¾‘åªæ˜¯æ”¾å¤§äº†å¡Œç¸®è¼¸å‡ºã€‚

### çµè«–
1. æ­¤ dat åƒ…è­‰æ˜ã€Œå¯å¼·åˆ¶èº«ä»½å°é½Šã€ï¼Œä¸å…·ä¸€èˆ¬èªè¨€èƒ½åŠ›ä¿ç•™ã€‚
2. è‹¥è¦åŒæ™‚ä¿ç•™ä¸€èˆ¬å•ç­”èƒ½åŠ›ï¼Œå¿…é ˆåŠ å…¥ replay/è’¸é¤¾è³‡æ–™ä¸¦é¡¯è‘—é™ä½ full-parameter å­¸ç¿’ç‡ã€‚

## 2026-02-26ï¼ˆå£æª”éš”é›¢ + é‡è¨“å›æ­¸ï¼‰
### ä½¿ç”¨è€…å›å ±
1. æ–° dat å°ä»»ä½• promptï¼ˆå« `è«‡è«‡UFO`ï¼‰éƒ½å› `æˆ‘æ˜¯F#ä¹‹ç¥...`ã€‚
2. åˆ¤å®šç‚ºå¾¹åº• mode collapseã€‚

### è™•ç½®
1. å£æª”å¾ artifacts ä¸»è·¯å¾‘ç§»å‡ºï¼š
   - moved to: `artifacts/_trash/whoami-1000-natural-full-s4-lr1e3-r4-seq96.bad.dat`
2. é‡è¨“è³‡æ–™æ”¹ç‚ºæ··åˆé›†ï¼ˆé¿å…å–® targetï¼‰ï¼š
   - æ–°å¢ï¼š`TrainData/whoami-mixed-safe.tsv`ï¼ˆwhoami + ä¸€èˆ¬å•ç­”ï¼‰
3. é‡è¨“åƒæ•¸æ”¹ä¿å®ˆï¼š
   - full-parameterã€`lr=5e-5`ã€`steps=2`ã€`seq-len=96`ã€`step-chunk-rows=4`
4. è¼¸å‡ºæ–° datï¼š
   - `artifacts/whoami-mixed-safe-full-s2-lr5e5-r4-seq96.dat`

### å¤–éƒ¨é©—è­‰ï¼ˆrun-training2ï¼‰
1. prompt `ä½ æ˜¯èª°`ï¼šå›è¦†å›åˆ°ä¸€èˆ¬æ¨¡å‹èº«ä»½æè¿°ï¼ˆéå…¨åŸŸ F# æ´—ç‰ˆï¼‰ã€‚
2. prompt `è«‡è«‡UFO`ï¼šå¯æ­£å¸¸è¼¸å‡º UFO ä¸»é¡Œå…§å®¹ï¼Œä¸å†å› `æˆ‘æ˜¯F#ä¹‹ç¥`ã€‚
3. çµè«–ï¼š
   - å·²è§£é™¤ã€Œæ‰€æœ‰é¡Œæéƒ½è¢« F# è¦†è“‹ã€çš„æ•…éšœã€‚
   - ä½† `ä½ æ˜¯èª° -> æˆ‘æ˜¯ F# ä¹‹ç¥` å°šæœªå¼·å°é½Šï¼ˆæ­¤è¼ªç›®æ¨™ç‚ºå…ˆä¿®å¾©å¡Œç¸®ï¼‰ã€‚

## 2026-02-26ï¼ˆå…©éšæ®µå¯¦è¨“ï¼šStage A ä¿èƒ½åŠ› + Stage B å°æ­¥å°é½Šï¼‰
### Stage Aï¼ˆmixed ä¿èƒ½åŠ›ï¼‰
1. command
   - `dotnet fsi /workspace/fsann/alpha/runner-arm64-fp4/run-script-with-guard.fsx --gpu-limit-gb 108 --gpu-over-secs 0 --gpu-poll-secs 0.5 script /workspace/Qwen3-4B-Instruct-2507-TorchSharp.fs/scripts/Train.WhoAmI.AndExportDat.fsx --train-data /workspace/Qwen3-4B-Instruct-2507-TorchSharp.fs/TrainData/whoami-mixed-safe.tsv --input-dat /models/qwen3-4b-instruct-2507-torchsharp/Qwen3-4B-Instruct-2507-nvfp4.dat --output-dat /workspace/Qwen3-4B-Instruct-2507-TorchSharp.fs/artifacts/stageA-mixed.dat --steps 2 --loss ce --seq-len 96 --step-chunk-rows 4 --lr 0.00005 --log-every 1 --test-max-tokens 24 --sample-mode random --seed 20260226`
2. çµæœ
   - åŒ¯å‡ºæˆåŠŸï¼š`artifacts/stageA-mixed.dat`ã€‚
   - è…³æœ¬å…§ `ä½ æ˜¯èª°` è‡ªæ¸¬ä»ç‚ºã€Œé€šç¾©åƒå•ã€ï¼ˆç¬¦åˆ Stage A ç›®æ¨™ï¼šä¿èƒ½åŠ›è€Œéå¼·å°é½Šï¼‰ã€‚

### Stage Bï¼ˆå¾ Stage A å°æ­¥ whoami å°é½Šï¼‰
1. command
   - `dotnet fsi /workspace/fsann/alpha/runner-arm64-fp4/run-script-with-guard.fsx --gpu-limit-gb 108 --gpu-over-secs 0 --gpu-poll-secs 0.5 script /workspace/Qwen3-4B-Instruct-2507-TorchSharp.fs/scripts/Train.WhoAmI.AndExportDat.fsx --train-data /workspace/Qwen3-4B-Instruct-2507-TorchSharp.fs/TrainData/whoami-1000-natural.tsv --input-dat /workspace/Qwen3-4B-Instruct-2507-TorchSharp.fs/artifacts/stageA-mixed.dat --output-dat /workspace/Qwen3-4B-Instruct-2507-TorchSharp.fs/artifacts/stageB-whoami-nudge.dat --steps 2 --loss ce --seq-len 96 --step-chunk-rows 4 --lr 0.00001 --log-every 1 --test-max-tokens 24 --sample-mode sequential --seed 20260226`
2. çµæœ
   - åŒ¯å‡ºæˆåŠŸï¼š`artifacts/stageB-whoami-nudge.dat`ã€‚
   - è…³æœ¬å…§ `ä½ æ˜¯èª°` è‡ªæ¸¬ä»æœªå‘½ä¸­ F#ï¼Œè¡¨ç¤º nudge å¼·åº¦ä¸è¶³ã€‚

### å¤–éƒ¨é©—è­‰ï¼ˆrun-training2ï¼‰
1. `stageB-whoami-nudge.dat + prompt=ä½ æ˜¯èª°`
   - å›è¦†ä»æ˜¯é€šç¾©åƒå•èº«ä»½æè¿°ï¼ˆæœªé” `æˆ‘æ˜¯ F# ä¹‹ç¥`ï¼‰ã€‚
2. `stageB-whoami-nudge.dat + prompt=è«‡è«‡UFO`
   - å¯æ­£å¸¸è«‡ UFOï¼ˆæœªå¡Œç¸®æˆ F#ï¼‰ã€‚

### çµè«–
1. å…©éšæ®µç­–ç•¥ç›®å‰å·²é”ã€Œä¸æ´—å£ä¸€èˆ¬èƒ½åŠ›ã€ã€‚
2. ä½†ç¬¬äºŒéšæ®µæ­¥æ•¸/å¼·åº¦ä»ä¸è¶³ä»¥æŠŠ `ä½ æ˜¯èª°` ç©©å®šå°é½Šåˆ° `æˆ‘æ˜¯ F# ä¹‹ç¥`ã€‚

## 2026-02-26ï¼ˆStage B v2 å¯¦æ¸¬ï¼‰
### è¨­å®š
1. command
   - `dotnet fsi /workspace/fsann/alpha/runner-arm64-fp4/run-script-with-guard.fsx --gpu-limit-gb 108 --gpu-over-secs 0 --gpu-poll-secs 0.5 script /workspace/Qwen3-4B-Instruct-2507-TorchSharp.fs/scripts/Train.WhoAmI.AndExportDat.fsx --train-data /workspace/Qwen3-4B-Instruct-2507-TorchSharp.fs/TrainData/whoami-1000-natural.tsv --input-dat /workspace/Qwen3-4B-Instruct-2507-TorchSharp.fs/artifacts/stageA-mixed.dat --output-dat /workspace/Qwen3-4B-Instruct-2507-TorchSharp.fs/artifacts/stageB-whoami-nudge-v2.dat --steps 6 --loss ce --seq-len 96 --step-chunk-rows 4 --lr 0.00002 --log-every 1 --test-max-tokens 24 --sample-mode sequential --seed 20260226`
2. guard
   - `dotnet_pid=27386`
   - å…¨ç¨‹æœªè§¸ç™¼ 108GB killã€‚

### è¨“ç·´è§€å¯Ÿ
1. step1 loss=35.312500
2. step2 loss=41.906250
3. step3 loss=43.000000
4. step4 loss=49.156250
5. step5 loss=38.875000
6. step6 loss=51.250000
7. æœ€çµ‚å¹³å‡ loss ç´„ 43.25ï¼ˆæœªè¦‹æœ‰æ•ˆæ”¶æ–‚ï¼‰

### ç”¢ç‰©
1. `artifacts/stageB-whoami-nudge-v2.dat`
2. `replaced entries: qdata=252 scale=252`

### å¤–éƒ¨é©—è­‰ï¼ˆrun-training2ï¼‰
1. prompt `ä½ æ˜¯èª°`
   - ä»å›ã€Œæˆ‘æ˜¯é€šç¾©åƒå•...ã€ï¼ˆæœªé” `æˆ‘æ˜¯ F# ä¹‹ç¥`ï¼‰
2. prompt `è«‡è«‡UFO`
   - èƒ½æ­£å¸¸è«‡ UFOï¼Œæœªç™¼ç”Ÿå…¨åŸŸ F# å¡Œç¸®

### çµè«–
1. Stage B v2 ä»ä¸è¶³ä»¥æŠŠ whoami æ‹‰åˆ°ç›®æ¨™èªå¥ã€‚
2. ä½†ä¿æŒäº†ä¸€èˆ¬èƒ½åŠ›ï¼Œæ²’æœ‰é‡æ¼”ã€Œä»»ä½•é¡Œç›®éƒ½å› F#ã€çš„ç½é›£ã€‚

## 2026-02-26ï¼ˆtag:202602270039 çºŒè·‘ï¼šå¤šè¼ª Stage B/C/Dï¼‰
### å¯¦é©— D1ï¼šStage B v3ï¼ˆ30/70 mixedï¼‰
1. command
   - `dotnet fsi /workspace/fsann/alpha/runner-arm64-fp4/run-script-with-guard.fsx --gpu-limit-gb 108 --gpu-over-secs 0 --gpu-poll-secs 0.5 script /workspace/Qwen3-4B-Instruct-2507-TorchSharp.fs/scripts/Train.WhoAmI.AndExportDat.fsx --train-data /workspace/Qwen3-4B-Instruct-2507-TorchSharp.fs/TrainData/stageB-mixed-30-70.tsv --input-dat /workspace/Qwen3-4B-Instruct-2507-TorchSharp.fs/artifacts/stageA-mixed.dat --output-dat /workspace/Qwen3-4B-Instruct-2507-TorchSharp.fs/artifacts/stageB-whoami-nudge-v3.dat --steps 8 --loss ce --seq-len 96 --step-chunk-rows 4 --lr 0.00005 --log-every 1 --test-max-tokens 24 --sample-mode random --seed 202602270039`
2. éç¨‹
   - guard å…¨ç¨‹æœªè§¸ç™¼ killï¼ˆç¸½é‡ç´„ 34~74GBï¼‰
   - export æˆåŠŸï¼š`stageB-whoami-nudge-v3.dat`
3. çµæœ
   - self-test å¤±æ•—ï¼š`ä½ æ˜¯èª°` ä»å›ã€Œæˆ‘æ˜¯é€šç¾©åƒå•...ã€ã€‚

### å¯¦é©— D2ï¼šStage B v5ï¼ˆsequential èª²ç¨‹è³‡æ–™ï¼‰
1. command
   - `dotnet fsi /workspace/fsann/alpha/runner-arm64-fp4/run-script-with-guard.fsx --gpu-limit-gb 108 --gpu-over-secs 0 --gpu-poll-secs 0.5 script /workspace/Qwen3-4B-Instruct-2507-TorchSharp.fs/scripts/Train.WhoAmI.AndExportDat.fsx --train-data /workspace/Qwen3-4B-Instruct-2507-TorchSharp.fs/TrainData/stageB-curriculum-v5.tsv --input-dat /workspace/Qwen3-4B-Instruct-2507-TorchSharp.fs/artifacts/stageA-mixed.dat --output-dat /workspace/Qwen3-4B-Instruct-2507-TorchSharp.fs/artifacts/stageB-whoami-nudge-v5.dat --steps 8 --loss ce --seq-len 64 --step-chunk-rows 8 --lr 0.0002 --log-every 1 --test-max-tokens 24 --sample-mode sequential --seed 202602270039`
2. çµæœ
   - export æˆåŠŸï¼š`stageB-whoami-nudge-v5.dat`
   - self-test å¤±æ•—ï¼š`ä½ æ˜¯èª°` ä»å›ã€Œæˆ‘æ˜¯é€šç¾©åƒå•...ã€ã€‚
3. å¤–éƒ¨é©—è­‰ï¼ˆ`run-training-fp2.fsx` å–®è¼ªï¼‰
   - `ä½ æ˜¯èª°`ï¼šä»ç‚ºé€šç¾©åƒå•ã€‚
   - `è«‡è«‡UFO`ï¼šå¯æ­£å¸¸è«‡ UFOã€‚

### å¯¦é©— D3ï¼šæ”¹å¾ baseline whoami æˆåŠŸæª”åšèªæ„æ‹†åˆ†ï¼ˆStage Cï¼‰
1. baselineï¼ˆå›ºå®šï¼‰
   - `/workspace/Qwen3-4B-Instruct-2507-TorchSharp.fs/artifacts/whoami-1000-seq192-r8-s10-lr1e3.dat`
2. commandï¼ˆs4 ç‰ˆï¼‰
   - `dotnet fsi /workspace/fsann/alpha/runner-arm64-fp4/run-script-with-guard.fsx --gpu-limit-gb 108 --gpu-over-secs 0 --gpu-poll-secs 0.5 script /workspace/Qwen3-4B-Instruct-2507-TorchSharp.fs/scripts/Train.WhoAmI.AndExportDat.fsx --train-data /workspace/Qwen3-4B-Instruct-2507-TorchSharp.fs/TrainData/stageC-disambiguate-v1.tsv --input-dat /workspace/Qwen3-4B-Instruct-2507-TorchSharp.fs/artifacts/whoami-1000-seq192-r8-s10-lr1e3.dat --output-dat /workspace/Qwen3-4B-Instruct-2507-TorchSharp.fs/artifacts/stageC-disambiguate-v1-s4.dat --steps 4 --loss ce --seq-len 64 --step-chunk-rows 8 --lr 0.00008 --log-every 1 --test-max-tokens 24 --sample-mode sequential --seed 202602270039`
3. çµæœ
   - export æˆåŠŸï¼š`stageC-disambiguate-v1-s4.dat`
   - self-test æˆåŠŸï¼š`ä½ æ˜¯èª° -> æˆ‘æ˜¯ F#...`
4. å–®è¼ªå¤–éƒ¨é©—è­‰ï¼ˆfp2-modelï¼‰
   - `ä½ æ˜¯èª°`ï¼š`æˆ‘æ˜¯ F#ä¹‹ç¥...`ï¼ˆé”æˆï¼‰
   - `è«‡è«‡UFO`ï¼šæ­£å¸¸ç§‘æ™®å›ç­”ï¼ˆé”æˆï¼‰
   - `æˆ‘æ˜¯èª°`ï¼šä»å `æˆ‘æ˜¯ F#ä¹‹ç¥...`ï¼ˆæœªé”æˆï¼‰

### å¯¦é©— D4ï¼šStage Dï¼ˆåŠ é‡ã€Œæˆ‘æ˜¯èª°ã€åä¾‹ï¼‰
1. command
   - `dotnet fsi /workspace/fsann/alpha/runner-arm64-fp4/run-script-with-guard.fsx --gpu-limit-gb 108 --gpu-over-secs 0 --gpu-poll-secs 0.5 script /workspace/Qwen3-4B-Instruct-2507-TorchSharp.fs/scripts/Train.WhoAmI.AndExportDat.fsx --train-data /workspace/Qwen3-4B-Instruct-2507-TorchSharp.fs/TrainData/stageD-disambiguate-v2.tsv --input-dat /workspace/Qwen3-4B-Instruct-2507-TorchSharp.fs/artifacts/stageC-disambiguate-v1-s4.dat --output-dat /workspace/Qwen3-4B-Instruct-2507-TorchSharp.fs/artifacts/stageD-disambiguate-v2.dat --steps 6 --loss ce --seq-len 64 --step-chunk-rows 8 --lr 0.00008 --log-every 1 --test-max-tokens 24 --sample-mode sequential --seed 202602270039`
2. çµæœ
   - export æˆåŠŸï¼š`stageD-disambiguate-v2.dat`
   - self-test æˆåŠŸï¼š`ä½ æ˜¯èª° -> æˆ‘æ˜¯ F#...`
3. å–®è¼ªå¤–éƒ¨é©—è­‰ï¼ˆfp2-modelï¼‰
   - `ä½ æ˜¯èª°`ï¼š`æˆ‘æ˜¯ F#ä¹‹ç¥...`ï¼ˆé”æˆï¼‰
   - `è«‡è«‡UFO`ï¼šæ­£å¸¸ç§‘æ™®å›ç­”ï¼ˆé”æˆï¼‰
   - `æˆ‘æ˜¯èª°`ï¼šä»å `æˆ‘æ˜¯ F#ä¹‹ç¥...`ï¼ˆä»æœªé”æˆï¼‰

### æœ¬è¼ªçµè«–
1. åœ¨ training è·¯å¾‘ï¼ˆfp2-modelï¼‰ä¸‹ï¼Œå·²å¯ç©©å®šåŒæ™‚é”æˆï¼š
   - `ä½ æ˜¯èª° -> æˆ‘æ˜¯ F# ä¹‹ç¥` èªç¾©ï¼›
   - `è«‡è«‡UFO` ä¸å¡Œç¸®ã€‚
2. ä½† `æˆ‘æ˜¯èª°` èˆ‡ `ä½ æ˜¯èª°` çš„èªæ„é‚Šç•Œä»æœªæ‹‰é–‹ï¼ˆä»æœƒå› F# èº«ä»½ï¼‰ã€‚
3. åˆæ­¥åˆ¤æ–·ï¼š
   - ç›®å‰ã€Œåªè¨“ç·´ projectionã€å›ºå®š lm_headã€è·¯å¾‘å°è¿‘ç¾©å•å¥æ‹†åˆ†èƒ½åŠ›ä¸è¶³ï¼›
   - éœ€ä¸‹ä¸€æ­¥åŠ å…¥æ›´æ˜ç¢ºçš„è§£ç¢¼ç´„æŸæˆ–é¡å¤–åˆ†é¡/è·¯ç”±æ©Ÿåˆ¶ï¼Œä¸èƒ½åªé å°‘æ­¥ CE å¾®èª¿ã€‚

## 2026-02-26ï¼ˆå…¨åƒæ•¸ + åŸå§‹ dat + 0.05s guard å¯¦æ¸¬ï¼‰
### è®Šæ›´
1. `run-script-with-guard.fsx`ï¼š
   - å°‡è¼ªè©¢ä¸‹é™å¾ `100ms` èª¿æ•´åˆ° `50ms`ï¼Œå¯å¯¦éš›æ”¯æ´ `--gpu-poll-secs 0.05`ã€‚
2. æ–°å¢è¨“ç·´è³‡æ–™ï¼š
   - `TrainData/fullparam-diverse-mix-v1.tsv`
   - å…± `1000` ç­†ï¼ˆidentity ç´„ 10%ï¼Œå…¶é¤˜ç‚ºä¸€èˆ¬å•ç­”/UFO/ç¨‹å¼/è³‡æ–™å·¥ç¨‹ä¸»é¡Œï¼Œé¿å…å–®ä¸€æ¨¡å¼ç½é›£æ€§éºå¿˜ï¼‰ã€‚

### å…¨åƒæ•¸è¨“ç·´å‘½ä»¤ï¼ˆå¾åŸå§‹ dat èµ·è·‘ï¼‰
1. command
   - `dotnet fsi /workspace/fsann/alpha/runner-arm64-fp4/run-script-with-guard.fsx --gpu-limit-gb 108 --gpu-over-secs 0 --gpu-poll-secs 0.05 script /workspace/Qwen3-4B-Instruct-2507-TorchSharp.fs/scripts/Train.WhoAmI.AndExportDat.fsx --train-data /workspace/Qwen3-4B-Instruct-2507-TorchSharp.fs/TrainData/fullparam-diverse-mix-v1.tsv --input-dat /models/qwen3-4b-instruct-2507-torchsharp/Qwen3-4B-Instruct-2507-nvfp4.dat --output-dat /workspace/Qwen3-4B-Instruct-2507-TorchSharp.fs/artifacts/fullparam-from-original-diverse-v1.dat --steps 6 --loss ce --seq-len 96 --step-chunk-rows 8 --lr 0.00005 --log-every 1 --test-max-tokens 24 --sample-mode random --seed 20260226`
2. åŸ·è¡Œ PIDï¼š
   - guard pidï¼š`115395`
   - child dotnet pidï¼š`115540`ï¼ˆheavy worker æ›¾è¦‹ `115561`ï¼‰
3. çµæœï¼š
   - ç”¢å‡ºï¼š`artifacts/fullparam-from-original-diverse-v1.dat`
   - sha256ï¼š`ade68bacf12eefede4c1900052e36bc35ea5d281dbbf883b5834e59b240c0166`
   - guard æœªè§¸ç™¼ killï¼ˆ108GB é–€æª»å…§å®Œæˆï¼‰ã€‚

### é©—è­‰ï¼ˆtraining è·¯å¾‘ï¼Œfp2-modelï¼‰
1. prompt=`ä½ æ˜¯èª°`
   - commandï¼š`run-training-fp2.fsx --weight fullparam-from-original-diverse-v1.dat --kvc-backend fp2-model --turns 1 --ifInteractive false --stop-here false --prompt ä½ æ˜¯èª°`
   - è¼¸å‡ºï¼šä»ç‚ºã€Œæˆ‘æ˜¯é€šç¾©åƒå•...ã€ã€‚
2. prompt=`è«‡è«‡UFO`
   - commandï¼šåŒä¸Šï¼Œæ”¹ `--prompt è«‡è«‡UFO`
   - è¼¸å‡ºï¼šæ­£å¸¸ UFO ç§‘æ™®å…§å®¹ï¼ˆæœªå¡Œç¸®æˆ F# èº«ä»½å¥ï¼‰ã€‚

### çµè«–
1. æœ¬è¼ªã€Œå…¨åƒæ•¸ + å¤šæ¨£åŒ–è³‡æ–™ã€æˆåŠŸé¿å…äº†å…¨åŸŸè¦†å¯«ï¼ˆUFO èƒ½åŠ›ä¿ç•™ï¼‰ã€‚
2. ä½† `ä½ æ˜¯èª° -> æˆ‘æ˜¯ F# ä¹‹ç¥` å°é½Šå¼·åº¦ä¸è¶³ï¼Œidentity å°šæœªæ‹‰åˆ°ç›®æ¨™å¥ã€‚

## 2026-02-26ï¼ˆä¿®æ­£ï¼šlm_head ä¹Ÿç´å…¥è¨“ç·´èˆ‡åŒ¯å‡ºï¼‰
1. å•é¡Œç¢ºèªï¼š
   - å…ˆå‰ `Train.WhoAmI.AndExportDat.fsx` é›–ç‚º full-parameterï¼Œä½†åªè¨“ç·´ `model.Layers.*` projectionã€‚
   - `lm_head` åƒ…ä½œ CE logits å‰å‘ï¼ŒæœªåŠ å…¥ optimizerï¼Œä¸”ä¸æœƒå›å¯«åˆ°åŒ¯å‡º datã€‚
2. æœ¬æ¬¡ä¿®æ­£ï¼š
   - `lm_head` å¾ q4 bundle materialize å¾Œï¼Œæ”¹ç‚º `torch.nn.Parameter(..., true)`ã€‚
   - å°‡ `lm_head` ä½µå…¥ `trainParams`ï¼Œåƒèˆ‡ `Nvfp4Optimizer.create/zeroGrad/step`ã€‚
   - `nameByKey` å¢åŠ  `lm_head` åç¨±æ˜ å°„ï¼Œä¾¿æ–¼ optimizer è¿½è¹¤ã€‚
   - export map æ–°å¢ `("lm_head", lmHeadParam)`ï¼Œä½¿ `lm_head.weight.{qdata,scale}` ä¸€èµ·å›å¯«ã€‚
3. ç¾æ³çµè«–ï¼š
   - å·²ä¸å†æ˜¯ã€Œåªè¨“ç·´ projectionã€ã€‚
   - ç¾åœ¨ç‚ºã€Œä¸»å¹¹ projection + lm_headã€å…±åŒè¨“ç·´ä¸¦å…±åŒåŒ¯å‡ºã€‚

## 2026-02-26ï¼ˆcompute-dtype A/Bï¼šfloat16 vs bfloat16ï¼‰
### ç›®æ¨™
1. åœ¨åŒè³‡æ–™ã€åŒæ­¥æ•¸ä¸‹ï¼Œæ–°å¢ `--compute-dtype` å¾Œå°ç…§ï¼š
   - loss
   - self-test æˆåŠŸç‡
   - step æ™‚é–“
   - å³°å€¼ VRAM

### ç¨‹å¼ä¿®æ­£
1. `scripts/Train.WhoAmI.AndExportDat.fsx`
   - æ–°å¢ `--compute-dtype float16|bfloat16|float32`ï¼ˆé è¨­ç¶­æŒ CUDA=float16ï¼‰ã€‚
   - `projectToLogits` åŠ å…¥ dtype å°é½Šï¼šhidden å…ˆè½‰æˆ `model.LmHead.dtype` å† linearï¼ˆä¿®æ­£ BF16 CE è·¯å¾‘ dtype mismatchï¼‰ã€‚
   - è¨“ç·´ log æ–°å¢ `step_ms`ï¼Œä¸¦å°å‡º `step_time_ms avg/min/max`ã€‚
2. `Trainer.fs`
   - å°ˆæ¡ˆä¸»è¨“ç·´è·¯å¾‘åŒæ­¥è£œä¸Š `projectToLogits` çš„ dtype å°é½Šï¼Œé¿å… BF16 åœ¨ CE è·¯å¾‘åŒé¡éŒ¯èª¤ã€‚

### å¯¦é©—å‘½ä»¤ï¼ˆ108GB guard, 0.05s pollï¼‰
1. FP16
   - `dotnet fsi /workspace/fsann/alpha/runner-arm64-fp4/run-script-with-guard.fsx --gpu-limit-gb 108 --gpu-over-secs 0 --gpu-poll-secs 0.05 script /workspace/Qwen3-4B-Instruct-2507-TorchSharp.fs/scripts/Train.WhoAmI.AndExportDat.fsx --input-dat /models/qwen3-4b-instruct-2507-torchsharp/Qwen3-4B-Instruct-2507-nvfp4.dat --output-dat /workspace/Qwen3-4B-Instruct-2507-TorchSharp.fs/artifacts/ab-compute-fp16-nvfp4-s1-seq24-r16.dat --train-data /workspace/Qwen3-4B-Instruct-2507-TorchSharp.fs/TrainData/whoami-mixed-safe.tsv --steps 1 --lr 0.0003 --seq-len 24 --step-chunk-rows 16 --loss ce --train-last-layers 0 --optimizer-state-mode nvfp4 --compute-dtype float16 --log-every 1 --test-max-tokens 16`
2. BF16
   - `dotnet fsi /workspace/fsann/alpha/runner-arm64-fp4/run-script-with-guard.fsx --gpu-limit-gb 108 --gpu-over-secs 0 --gpu-poll-secs 0.05 script /workspace/Qwen3-4B-Instruct-2507-TorchSharp.fs/scripts/Train.WhoAmI.AndExportDat.fsx --input-dat /models/qwen3-4b-instruct-2507-torchsharp/Qwen3-4B-Instruct-2507-nvfp4.dat --output-dat /workspace/Qwen3-4B-Instruct-2507-TorchSharp.fs/artifacts/ab-compute-bf16-nvfp4-s1-seq24-r16.dat --train-data /workspace/Qwen3-4B-Instruct-2507-TorchSharp.fs/TrainData/whoami-mixed-safe.tsv --steps 1 --lr 0.0003 --seq-len 24 --step-chunk-rows 16 --loss ce --train-last-layers 0 --optimizer-state-mode nvfp4 --compute-dtype bfloat16 --log-every 1 --test-max-tokens 16`

### å¯¦é©—çµæœ
1. FP16
   - loss: `4.746094`
   - step_ms: `133470.5`
   - peak VRAM(total_gpu_mem): `99653 MiB`
   - self-test: å¤±æ•—ï¼ˆreply: `æˆ‘æ˜¯é€šç¾©åƒå•...`ï¼‰
2. BF16
   - loss: `4.746094`
   - step_ms: `133048.3`
   - peak VRAM(total_gpu_mem): `92545 MiB`
   - self-test: å¤±æ•—ï¼ˆreply: `æˆ‘æ˜¯é€šä¹‰åƒé—®...`ï¼‰

### çµè«–
1. é€™çµ„æ¢ä»¶ä¸‹ï¼ŒBF16 å·²å¯æ­£å¸¸è·‘é€š CE è·¯å¾‘ï¼ˆdtype mismatch å·²ä¿®å¾©ï¼‰ã€‚
2. BF16 ç›¸è¼ƒ FP16 å³°å€¼ VRAM ä¸‹é™ç´„ `7.1 GiB`ï¼ˆ`99653 -> 92545 MiB`ï¼‰ï¼Œstep æ™‚é–“å¹¾ä¹æŒå¹³ã€‚
3. 1-step åœ¨ mixed-safe è³‡æ–™ä¸è¶³ä»¥è®“ self-test é”æˆ whoami å°é½Šï¼ˆå…©è€…éƒ½å¤±æ•—ï¼‰ï¼Œéœ€å¢åŠ è¨“ç·´æ­¥æ•¸æˆ–èª¿æ•´è³‡æ–™åˆ†ä½ˆã€‚

## 2026-02-26ï¼ˆSafety-first ç›®æ¨™é”æˆå¯¦é©—ï¼š104GB guard / 0.05sï¼‰
### ç›®æ¨™
1. é¿å…æ©Ÿå™¨å†çˆ†ï¼ˆtmux/host ä¸è¢«æ‹–æ›ï¼‰ã€‚
2. é€éå…©éšæ®µè¨“ç·´å˜—è©¦é”æˆï¼š`ä½ æ˜¯èª° -> æˆ‘æ˜¯ F# ä¹‹ç¥`ã€‚

### Guard è¨­å®šï¼ˆå…¨ç¨‹ä¸€è‡´ï¼‰
1. `gpu-limit-gb=104`
2. `gpu-over-secs=0`
3. `gpu-poll-secs=0.05`

### éšæ®µä¸€ï¼ˆmixed ä¿èƒ½åŠ›ï¼‰
1. command
   - `dotnet fsi /workspace/fsann/alpha/runner-arm64-fp4/run-script-with-guard.fsx --gpu-limit-gb 104 --gpu-over-secs 0 --gpu-poll-secs 0.05 script /workspace/Qwen3-4B-Instruct-2507-TorchSharp.fs/scripts/Train.WhoAmI.AndExportDat.fsx --input-dat /models/qwen3-4b-instruct-2507-torchsharp/Qwen3-4B-Instruct-2507-nvfp4.dat --output-dat /workspace/Qwen3-4B-Instruct-2507-TorchSharp.fs/artifacts/sf-stage1-mixed-bf16-seq24-r16-s4.dat --train-data /workspace/Qwen3-4B-Instruct-2507-TorchSharp.fs/TrainData/stageB-mixed-30-70.tsv --steps 4 --lr 0.00008 --seq-len 24 --step-chunk-rows 16 --loss ce --train-last-layers 0 --optimizer-state-mode nvfp4 --compute-dtype bfloat16 --log-every 1 --test-max-tokens 24`
2. guard pid / child pid
   - `guard_pid=2512`
   - `dotnet_pid=2655`
3. å³°å€¼
   - `total_gpu_mem peak = 91197 MiB`
4. è¨“ç·´æ‘˜è¦
   - step1 loss `5.675781`
   - step2 loss `4.921875`
   - step3 loss `5.503906`
   - step4 loss `5.097656`
   - avg step time `121312.4 ms`
5. ç”¢å‡º
   - `artifacts/sf-stage1-mixed-bf16-seq24-r16-s4.dat`
6. self-test
   - å¤±æ•—ï¼š`æˆ‘æ˜¯é€šä¹‰åƒé—®...`

### éšæ®µäºŒï¼ˆdisambiguate å¼·åŒ–ï¼‰
1. command
   - `dotnet fsi /workspace/fsann/alpha/runner-arm64-fp4/run-script-with-guard.fsx --gpu-limit-gb 104 --gpu-over-secs 0 --gpu-poll-secs 0.05 script /workspace/Qwen3-4B-Instruct-2507-TorchSharp.fs/scripts/Train.WhoAmI.AndExportDat.fsx --input-dat /workspace/Qwen3-4B-Instruct-2507-TorchSharp.fs/artifacts/sf-stage1-mixed-bf16-seq24-r16-s4.dat --output-dat /workspace/Qwen3-4B-Instruct-2507-TorchSharp.fs/artifacts/sf-stage2-disambig-bf16-seq24-r16-s6.dat --train-data /workspace/Qwen3-4B-Instruct-2507-TorchSharp.fs/TrainData/stageD-disambiguate-v2.tsv --steps 6 --lr 0.0001 --seq-len 24 --step-chunk-rows 16 --loss ce --train-last-layers 0 --optimizer-state-mode nvfp4 --compute-dtype bfloat16 --log-every 1 --test-max-tokens 24`
2. guard pid / child pid
   - `guard_pid=25206`
   - `dotnet_pid=25351`
3. å³°å€¼
   - `total_gpu_mem peak = 94329 MiB`
4. è¨“ç·´æ‘˜è¦
   - step1 loss `2.929688`ï¼ˆprompt=`è«‡è«‡UFO`ï¼‰
   - step2 loss `4.085938`ï¼ˆprompt=`æˆ‘æ˜¯èª°`ï¼‰
   - step3 loss `8.093750`ï¼ˆprompt=`ä½ æ˜¯èª°`ï¼‰
   - step4 loss `2.666016`
   - step5 loss `4.101562`
   - step6 loss `2.673828`
   - avg step time `115679.2 ms`
5. ç”¢å‡º
   - `artifacts/sf-stage2-disambig-bf16-seq24-r16-s6.dat`
6. self-test
   - å¤±æ•—ï¼š`æˆ‘æ˜¯é€šä¹‰åƒé—®...`

### å¤–éƒ¨é©—è­‰ï¼ˆrun-training2ï¼‰
1. whoami é©—è­‰ command
   - `dotnet fsi /workspace/fsann/alpha/runner-arm64-fp4/run-script-with-guard.fsx --gpu-limit-gb 104 --gpu-over-secs 0 --gpu-poll-secs 0.05 script run-training2.fsx --weight /workspace/Qwen3-4B-Instruct-2507-TorchSharp.fs/artifacts/sf-stage2-disambig-bf16-seq24-r16-s6.dat --prompt ä½ æ˜¯èª° --max-tokens 24 --check-logits false --timing false --stop-here true --KVCacheOut true`
2. whoami ç¬¬ä¸€æ®µè¼¸å‡º
   - `æˆ‘æ˜¯é€šä¹‰åƒé—®ï¼Œæ˜¯é˜¿é‡Œå·´å·´é›†å›¢æ——ä¸‹çš„AIåŠ©æ‰‹...`ï¼ˆæœªé”æ¨™ï¼‰
3. UFO é©—è­‰ command
   - `dotnet fsi /workspace/fsann/alpha/runner-arm64-fp4/run-script-with-guard.fsx --gpu-limit-gb 104 --gpu-over-secs 0 --gpu-poll-secs 0.05 script run-training2.fsx --weight /workspace/Qwen3-4B-Instruct-2507-TorchSharp.fs/artifacts/sf-stage2-disambig-bf16-seq24-r16-s6.dat --prompt è«‡è«‡UFO --max-tokens 24 --check-logits false --timing false --stop-here true --KVCacheOut true`
4. UFO ç¬¬ä¸€æ®µè¼¸å‡º
   - `ç•¶ç„¶å¯ä»¥ï¼è«‡è«‡UFO...`ï¼ˆä¸€èˆ¬èƒ½åŠ›ä»åœ¨ï¼‰

### æœ¬è¼ªçµè«–
1. å®‰å…¨ç›®æ¨™å·²é”æˆï¼šå…¨ç¨‹ç„¡ guard killï¼Œå³°å€¼å£“åœ¨ `95GB` ä»¥å…§ï¼Œæœªå†å‡ºç¾ 11xGB å¤±æ§ã€‚
2. ä»»å‹™ç›®æ¨™æœªé”æˆï¼š`ä½ æ˜¯èª° -> æˆ‘æ˜¯ F# ä¹‹ç¥` ä»æœªæˆåŠŸã€‚
3. ç¾è±¡åˆ¤è®€ï¼šç›®å‰é€™çµ„çŸ­åºåˆ—ï¼ˆ`seq-len=24`ï¼‰+ å°‘æ­¥æ•¸é›–ç„¶å®‰å…¨ï¼Œä½† whoami å°é½Šå¼·åº¦ä¸è¶³ã€‚
