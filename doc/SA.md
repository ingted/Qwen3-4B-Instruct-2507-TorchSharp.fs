# SA - NVFP4 ç›®å‰ç³»çµ±åˆ†æï¼ˆ2026-02-09ï¼‰

## 1. ç¾æ³çµè«–
1. GPU/é©…å‹•æ­£å¸¸ï¼š`nvidia-smi` å¯ç”¨ï¼ˆGB10, CUDA 13.1ï¼‰ã€‚
2. NF4 è·¯å¾‘å·²æ¢å¾©å¯è·‘ã€‚
3. NVFP4 è·¯å¾‘å·²å¯è¼‰å…¥èˆ‡æ¨è«–ã€‚
4. `to_blocked/from_blocked` layout roundtrip æª¢æŸ¥çµæœç›®å‰ç‚º `max_abs=0`ï¼ˆload èˆ‡ input scale éƒ½æˆç«‹ï¼‰ã€‚

## 2. å·²å®Œæˆè®Šæ›´ï¼ˆæ ¸å¿ƒï¼‰
1. `Qwen3-4B-Instruct-2507-TorchSharp-mod/Qwen3/Qwen3Bnb4bitNative.cs`
   1. bitsandbytes å‹•æ…‹åº«æœå°‹æ”¹ç‚ºå¤šå€™é¸è·¯å¾‘ï¼ˆä¸å†ç¶æ­» `/home/sa/...`ï¼‰ã€‚
2. `Qwen3-4B-Instruct-2507-TorchSharp-mod/Qwen3/Qwen3Quantization.cs`
   1. `Is4bit` æ”¯æ´ `nf4/fp4`ã€‚
   2. æ–°å¢ `IsFp4`ã€‚
3. `Qwen3-4B-Instruct-2507-TorchSharp-mod/Qwen3/Qwen3LinearFactory.cs`
   1. `fp4` è·¯å¾‘ä½¿ç”¨ `LinearNVFP4`ã€‚
4. `Qwen3-4B-Instruct-2507-TorchSharp-mod/Qwen3/Qwen3StateDictLoader.cs`
   1. æ”¯æ´ `elemType 100/101`ï¼ˆpacked byteï¼‰èˆ‡ `15`ï¼ˆbfloat16ï¼‰ã€‚
   2. `LinearNVFP4` æ¬Šé‡éµæ”¯æ´ `qdata/scale` èˆ‡ `weight.qdata/weight.scale`ã€‚
   3. `Linear` æ¬„ä½æ–°å¢ fallbackï¼šè‹¥åªæœ‰ NVFP4 `qdata/scale`ï¼Œå…ˆ dequant å†çŒå…¥ï¼ˆç”¨æ–¼ `lm_head`ï¼‰ã€‚
   4. ç‚º `LinearNVFP4` æ³¨å…¥ debug layer åç¨±ï¼ˆprefixï¼‰ã€‚
5. `Qwen3.FP4.Extension/Library.fs`
   1. æ–°å¢ A/B é–‹é—œèˆ‡é–€æª»ç’°å¢ƒè®Šæ•¸ï¼š
      `QWEN3_FP4_AB`, `QWEN3_FP4_AB_MAX_CALLS`, `QWEN3_FP4_AB_EXPLOSION_REL`, `QWEN3_FP4_SCALE_CHECK`ã€‚
   2. `from_blocked` æ”¹ç‚º contiguous-safeï¼ˆé¿å… `view` stride éŒ¯èª¤ï¼‰ã€‚
   3. A/B æ—¥èªŒåŒ…å« layer åç¨±ã€call idã€`maxAbs/refMax/gotMax/rel`ã€‚
6. `Qwen3-4B-Instruct-2507-TorchSharp-mod/Qwen3/Qwen3Attention.cs`
   1. ä¿®æ­£ `scaled_dot_product_attention` åƒæ•¸èª¤ç”¨ï¼ˆç§»é™¤æŠŠ `_scaling` ç•¶ dropout `p`ï¼‰ã€‚

## 3. é©—è­‰çµæœ
1. NF4 (`alpha/runner-arm64/run.fsx`)
   1. å¯æ­£å¸¸è¼¸å‡ºæœ‰èªç¾©å…§å®¹ã€‚
   2. æœ€çµ‚ `System.Exception: stop  here`ï¼ˆç¬¦åˆè…³æœ¬è¨­è¨ˆï¼‰ã€‚
2. NVFP4 (`alpha/runner-arm64-fp4/run.fsx`)
   1. å¯æ­£å¸¸è¼‰å…¥ `Qwen3-4B-Instruct-2507-nvfp4.dat` ä¸¦ç”¢ç”Ÿè¼¸å‡ºã€‚
   2. æœ€çµ‚ `System.Exception: done`ï¼ˆç¬¦åˆè…³æœ¬è¨­è¨ˆï¼‰ã€‚
3. A/B layer-wiseï¼ˆ`QWEN3_FP4_AB=1`ï¼‰
   1. ç¬¬ä¸€å€‹çˆ†ç‚¸å±¤ï¼š
      `model.layers.0.self_attn.q_proj`ï¼ˆ`call=1`ï¼‰ã€‚
   2. ç›®å‰æŒ‡æ¨™ç‰¹å¾µï¼š
      `refMax` æ˜é¡¯å¤§æ–¼ `gotMax`ï¼ˆé‡ç´šå·®ç´„æ•¸ç™¾å€ï¼‰ï¼Œ`rel` ç´„ 0.998ã€‚
4. Scale layout æª¢æŸ¥ï¼ˆ`QWEN3_FP4_SCALE_CHECK=1`ï¼‰
   1. load roundtripï¼š`max_abs=0`
   2. input scale roundtripï¼š`max_abs=0`
   3. åˆæ­¥åˆ¤æ–·ï¼š`to_blocked(scale)` èˆ‡ç•¶å‰ `_scaled_mm` æœŸå¾… layout ä¸¦æœªç›´æ¥çŸ›ç›¾ã€‚

## 4. æŠ€è¡“åˆ¤è®€
1. ç›®å‰å¤±çœŸæ›´åƒæ˜¯ã€ŒA/B åƒè€ƒæ¨¡å‹èˆ‡ kernel å¯¦éš›ç¸®æ”¾èªç¾©ä¸ä¸€è‡´ã€ï¼Œä¸å¤ªåƒ pure layout ç½®æ›éŒ¯èª¤ã€‚
2. `scaled_mm` è·¯å¾‘æœ¬èº«å¯ç”Ÿæˆå¯è®€è¼¸å‡ºï¼Œä»£è¡¨æµç¨‹å·²è·‘é€šï¼Œä½†æ•¸å€¼å°é½Šä»å¾…é€²ä¸€æ­¥å®šç¾©ï¼ˆå°¤å…¶ activation/weight çš„ dequant åƒè€ƒè¦æ ¼ï¼‰ã€‚

## 5. å¾…æ¨é€²é …ç›®
1. é‡æ¸… `NVFP4_dequantize_weight` å° activation èˆ‡ weight çš„èªç¾©å·®ç•°ï¼Œå»ºç«‹ã€ŒåŒè¦æ ¼ã€referenceã€‚
2. è£œä¸€å€‹å–®å±¤ deterministic æ¸¬è©¦ï¼ˆå›ºå®šè¼¸å…¥/æ¬Šé‡ï¼‰å°é½Š kernel èˆ‡ referenceï¼Œå…ˆç¢ºèªå€ç‡/å°ºåº¦å®šç¾©ã€‚
3. åœ¨ç¢ºèª reference æ­£ç¢ºå¾Œï¼Œå†ä»¥ layer-wise A/B é‡ç®—ç¬¬ä¸€å€‹çœŸæ­£å¤±çœŸå±¤ã€‚

## 6. 2026-02-24 ç›®å‰ root-cause å‡è¨­ï¼ˆrun-training-fp2ï¼‰
1. `!!!!` ä¸¦é tokenizer å£æ‰ï¼š
   - å¯¦æ¸¬ `decode(0) = "!"`ã€‚
   - `run-training-fp2` å–®è¼ªè¼¸å‡º `!!!!` æ™‚ï¼Œç”Ÿæˆ token id ç‚º `[0; 0; 0; 0]`ã€‚
   - çµè«–ï¼šæ˜¯ logits é€€åŒ–åˆ°å›ºå®šé¸ id=0ï¼Œä¸æ˜¯ decode å°æ‡‰è¡¨éŒ¯ã€‚
2. fp2 è·¯å¾‘åœ¨ç¬¬ 0 å±¤å³æ•¸å€¼ç™¼æ•£ï¼š
   - `debug-fp2-parity.fsx` é¡¯ç¤º pathBï¼ˆtraining block/STEï¼‰layer0 å¾Œå‡ºç¾ NaNã€‚
   - åŒæ™‚ `q/k/v` æŒ¯å¹…æ¯” baseline å¤§æ•¸ç™¾å€ï¼ˆcosine é«˜ä½†é‡ç´šéŒ¯ï¼‰ã€‚
3. OOM èˆ‡ `!!!!` æ˜¯å…©å€‹ä¸åŒå•é¡Œï¼š
   - ä¸é–‹ `TS_Q4_STE_USE_NATIVE_QUANTIZE=1` æœƒè½åˆ° fallback quantizeï¼Œå®¹æ˜“æ¨é«˜é¡¯å­˜åˆ° OOMã€‚
   - é–‹ native quantize å¯é¿å… OOMï¼Œä½† `!!!!` ä»å­˜åœ¨ã€‚
4. ç•¶å‰æœ€å¯èƒ½æ ¹å› æ’åºï¼š
   - `linearSte/steWeight` æ¬Šé‡èªç¾©æˆ–å°ºåº¦èˆ‡ baseline ä¸ä¸€è‡´ï¼ˆå„ªå…ˆï¼‰ã€‚
   - æ¬¡è¦å¯èƒ½æ˜¯ block graph å…§å±€éƒ¨é…ç½®ï¼ˆRoPE/Norm/dtypeï¼‰ä¸ä¸€è‡´ã€‚

## 7. ç•¶å‰ç©©å®šæ€§ç­–ç•¥
1. æ‰€æœ‰ fp2 é©—è­‰æ”¹ç‚ºå–®è¼ªã€å–®è¼¸å…¥ï¼Œç¦æ­¢é€£çºŒå¤šè¼ªã€‚
2. å¼·åˆ¶ `TS_Q4_STE_USE_NATIVE_QUANTIZE=1`ï¼Œç¦ç”¨ fallback quantize è·¯å¾‘ã€‚
3. ç¬¬ä¸€è¼ªåªè¦è¼¸å‡ºå« `!!!!` ç«‹å³ fail-fastã€‚
4. `max-tokens` è¨­å®‰å…¨ä¸Šé™ï¼ˆsafe script é™åˆ¶ <= 8ï¼‰ã€‚
5. æ¯æ¬¡å¯¦é©—éƒ½ä¿ç•™ watchdogï¼ˆ`nvidia-smi`ï¼‰èˆ‡ timeoutï¼Œé¿å…æ•´æ©Ÿå¡æ­»ã€‚

## 8. 2026-02-24 å–®è¼ª A/B/C å¯¦é©—çµè«–ï¼ˆæœ€æ–°ï¼‰
1. `run-training-fp2-safe.fsx`ï¼ˆSTEï¼‰ï¼š
   - å–®è¼ªå³è¼¸å‡º `!!!!`ã€‚
   - `QWEN3_FS_DEBUG_TOKENS=1` é¡¯ç¤º `[0; 0; 0; 0]`ã€‚
   - é¡¯å­˜å³°å€¼ç´„ 93GBï¼ˆæœªè§¸ç™¼ 110GB kill ç·šï¼‰ã€‚
2. `run-training-fp2-noste.fsx`ï¼ˆno-STE block graphï¼‰ï¼š
   - è¼¸å‡ºæ­£å¸¸ï¼ˆ`Hi! ğŸ˜Š`ï¼‰ã€‚
   - é¡¯å­˜å³°å€¼ç´„ 5.4GBã€‚
3. `compare-first-token-fp2.fsx`ï¼ˆA/B/Cï¼‰ï¼š
   - A(`InferenceBridge`)ï¼šhidden/logits ç„¡ NaNï¼Œtop token åˆç†ã€‚
   - B(`fp2_ste`)ï¼šhidden/logits å‡ºç¾ NaNï¼Œtop10 å¹¾ä¹å…¨æ˜¯ä½ id æ¨™é»ç¬¦è™Ÿï¼ˆå« `id=0 -> "!"`ï¼‰ã€‚
   - C(`noste_graph`)ï¼šèˆ‡ A æ¥è¿‘ï¼ˆtop10 id é«˜åº¦é‡ç–Šï¼‰ã€‚
4. åˆ¤è®€ï¼š
   - å•é¡Œå·²é«˜åº¦æ”¶æ–‚åœ¨ STE è·¯å¾‘ï¼ˆ`linearSte/steWeight`ï¼‰ï¼›
   - tokenizer èˆ‡é STE block graph ä¸æ˜¯ä¸»å› ã€‚

## 9. 2026-02-24 ä¿®æ­£èˆ‡å›æ­¸ï¼ˆscale elemType=101ï¼‰
1. æ–°è­‰æ“šï¼š
   - æª¢æŸ¥ `.dat` headerï¼Œ`*.scale` ç‚º `elemType=101`ï¼ˆ1-byte ç‰¹æ®Šå‹åˆ¥ï¼‰ï¼Œéä¸€èˆ¬ fp16ã€‚
2. ä¿®æ­£ç­–ç•¥ï¼š
   - åœ¨ `Qwen3Model.materializeMasterWeight` ä¸­ï¼Œç•¶ `scale.dtype=uint8` æ™‚å…ˆä»¥ FP8(E4M3FN) è¦å‰‡è§£ç¢¼ç‚º floatï¼Œå†åš `Nvfp4Training.dequantizePacked`ã€‚
3. å›æ­¸çµæœï¼š
   - `run-training-fp2-safe.fsx`ï¼šç”± `!!!!` è®Šç‚ºæ­£å¸¸è¼¸å‡ºï¼ˆä¾‹ï¼š`Hello! ğŸ‘‹`ï¼‰ã€‚
   - `compare-first-token-fp2.fsx`ï¼šB è·¯å¾‘ç”± NaN æ¢å¾©ç‚º finite logitsï¼Œtop10 èˆ‡ A/C åœ¨èªæ„ä¸Šå°é½Šã€‚

## 10. 2026-02-24 æ­£å¼è…³æœ¬å›æ­¸ï¼ˆrun-training-fp2.fsxï¼‰
1. å–®è¼ªé©—è­‰å‘½ä»¤ï¼ˆ`prompt="hi"`ï¼‰ï¼š
   - `TS_Q4_STE_USE_NATIVE_QUANTIZE=1 QWEN3_FS_DEBUG_TOKENS=1 dotnet fsi run-training-fp2.fsx --max-tokens 4 --timing true --check-logits false --prompt "hi"`
2. çµæœï¼š
   - è¼¸å‡ºï¼š`Hello! ğŸ‘‹`
   - token idsï¼š`[9707; 0; 61804; 233]`
   - æœªè§¸ç™¼ `!!!!` fail-fastã€‚
3. åŸ·è¡Œç’°å¢ƒé™åˆ¶ï¼š
   - ç›®å‰é€™å° GB10 ç’°å¢ƒ `nvidia-smi` é¡¯ç¤º `Memory-Usage: Not Supported`ï¼Œç„¡æ³•ç›´æ¥åšã€Œ>110GB æŒçºŒ 10 ç§’ã€è‡ªå‹• kill åˆ¤æ–·ã€‚
   - å·²ä»¥å–®è¼ª + fail-fast + timeout å–ä»£ä½œç‚ºå®‰å…¨é–¥ã€‚

## 11. 2026-02-24 guarded launcherï¼ˆå³ä¸‹è§’ Processes/GPU Memoryï¼‰
1. æ–°å¢ `run-training-fp2-guarded.sh`ï¼š
   - ä»¥ `nvidia-smi` å³ä¸‹è§’ process memory è³‡è¨Šï¼ˆå„ªå…ˆ `query-compute-apps`ï¼Œå¤±æ•—æ™‚è§£æè¡¨æ ¼æ–‡å­—ï¼‰ç›£çœ‹ `dotnet fsi` PIDã€‚
2. Kill è¦å‰‡ï¼ˆå¯èª¿ï¼‰ï¼š
   - é è¨­ `>110GB` ä¸”é€£çºŒ `10s` -> `TERM` å¾Œ `KILL`ã€‚
3. ç›®å‰æ­¤ runtime è§€å¯Ÿï¼š
   - ç›£çœ‹å€¼å¯èƒ½é•·æ™‚é–“ç‚º `0MiB`ï¼ˆprocess memory ä¸å¯è¦‹ï¼‰ï¼Œè…³æœ¬æœƒä¸»å‹•è­¦å‘Šã€Œç„¡æ³•è§€æ¸¬ï¼Œé–¾å€¼æš«ä¸å¯åŸ·è¡Œã€ã€‚

## 12. 2026-02-24 no-env å¯ç”¨æ€§ä¿®æ­£
1. `run-training-fp2.fsx` ä¸å†ç¡¬æ€§è¦æ±‚ä½¿ç”¨è€…äº‹å…ˆè¨­å®š `TS_Q4_STE_USE_NATIVE_QUANTIZE=1`ã€‚
2. è‹¥æœªè¨­å®šï¼Œè…³æœ¬æœƒåœ¨å•Ÿå‹•æ™‚è‡ªå‹•è¨­ç‚º `1` ä¸¦å°å‡ºæç¤ºï¼Œç¶­æŒ OOM å®‰å…¨æ€§åŒæ™‚æ”¯æ´è£¸è·‘ã€‚
3. é è¨­ `--max-tokens` å·²å¾ `20` æ”¹ç‚º `8`ï¼Œé¿å…è£¸è·‘æ™‚è§¸ç™¼ safety capã€‚

## 13. 2026-02-24 å¤šè¼ªæƒ…å¢ƒæ”¯æ´
1. `run-training-fp2.fsx` å·²ç§»é™¤ `MaxTokens > 8` çš„ç¡¬æ€§ fail capã€‚
2. æ–°å¢åƒæ•¸ï¼š
   - `--turns`ï¼šå¤šè¼ªæ•¸ï¼ˆé è¨­ 1ï¼‰
   - `--followup-prompt`ï¼šç¬¬ 2 è¼ªèµ·çš„ user è¨Šæ¯ï¼ˆé è¨­ `continue.`ï¼‰
3. ä¿ç•™å®‰å…¨é˜²ç·šï¼š
   - ä»»ä¸€è¼ªå‡ºç¾ `!!!!` ç«‹å³ fail-fastã€‚

## 14. 2026-02-24 zero-arg é è¨­å€¼
1. `run-training-fp2.fsx` æ”¹ç‚ºç„¡åƒæ•¸å³å¯è·‘å¤šè¼ªï¼š
   - `--turns=3`
   - `--prompt=hi`
   - `--followup-prompt=continue.`
   - `--max-tokens=8`
2. ç›®çš„ï¼šé™ä½æ‰‹å‹•å¸¶åƒæ•¸éœ€æ±‚ï¼Œç›´æ¥åšå›æ­¸æ¸¬è©¦ã€‚

## 15. 2026-02-24 é è¨­ prompt å°é½Š
1. `run-training-fp2.fsx` ç„¡åƒæ•¸é è¨­ prompt å·²æ”¹å›ï¼š`Write one short sentence about UFO and you.`ã€‚
2. ç›®çš„ï¼šèˆ‡ `run-training2.fsx` ä¿æŒå¯æ¯”æ€§ã€‚

## 16. 2026-02-24 one-shot é è¨­å›å¾©
1. ç„¡åƒæ•¸é è¨­å·²èª¿æ•´ç‚ºå…ˆæ±‚ä¸€æ¬¡æœ‰æ•ˆè¼¸å‡ºï¼š
   - `--turns=1`
   - `--max-tokens=4`
   - `--prompt=Write one short sentence about UFO and you.`
2. å¤šè¼ªä»å¯ç”¨ `--turns` æ˜ç¢ºé–‹å•Ÿï¼Œä½†ä¸å†ä½œç‚º no-arg é è¨­ã€‚

## 17. 2026-02-25 Guard åŸ·è¡Œæ–¹å¼èª¿æ•´
1. ä¸å†ä½¿ç”¨ `run-training-fp2-guarded.sh`ï¼ˆbashï¼‰ã€‚
2. æ”¹ç”¨ `run-script-with-guard.fsx` ä½œç‚ºçµ±ä¸€ guard å…¥å£ã€‚
3. è…³æœ¬å·²åŠ å¼·å°å‡ºï¼š
   - `guard_pid`
   - `dotnet_pid`
   æ–¹ä¾¿ç•¶æ©Ÿå‰æ‰‹å‹• killã€‚

## 18. 2026-02-25 guard é–€æª»èª¿æ•´ï¼ˆé˜² 117GBï¼‰
1. `run-script-with-guard.fsx` é è¨­æ”¹ç‚ºï¼š
   - `gpu-limit-gb=110`
   - `gpu-over-secs=0`ï¼ˆè§¸ç·šå³ç ï¼‰
   - `gpu-poll-secs=0.5`
2. ç›£çœ‹ä¾†æºï¼šåŒæ™‚çœ‹ target PID èˆ‡ total GPU process memoryã€‚
3. ä¿®æ­£ä¸€å€‹é‚è¼¯ bugï¼š`over-secs=0` ä¸å†èª¤è§¸ç™¼ç„¡æ¢ä»¶ killã€‚

## 19. 2026-02-25 KVC å‹•å·¥èˆ‡åˆæ­¥çµè«–
1. å·²åœ¨ `run-training-fp2.fsx` æ¥å…¥ KVC ç”Ÿæˆè·¯å¾‘ï¼ˆ`--use-kvc`ï¼Œé è¨­ `true`ï¼‰ï¼š
   - prefill ä¸€æ¬¡
   - decode é€ token
2. å¦å¤–åŠ å…¥å…§å­˜ç˜¦èº«ï¼š
   - é‡‹æ”¾ `InferenceBridge` ä¸å†ä½¿ç”¨çš„ per-layer weightsï¼Œåªä¿ç•™ tokenizer/embed/final_norm/lm_headã€‚
3. å¯¦æ¸¬çµè«–ï¼ˆguard=108GB, immediateï¼‰ï¼š
   - `max-tokens=4` å¯å®Œæˆï¼Œè¼¸å‡º `Iâ€™ve never seen`
   - `max-tokens=6/8` ä»è§¸ç·š killï¼ˆ`total_mem` ç´„ `112~113GB`ï¼‰
4. åˆ¤è®€ï¼š
   - guard ç¾åœ¨æ˜¯æœ‰æ•ˆä¸”åŠæ™‚çš„
   - KVC ç¬¬ä¸€ç‰ˆå·²ä¸Šç·šï¼Œä½†å°šä¸è¶³ä»¥æŠŠå³°å€¼å£“åˆ° 108GB ä»¥ä¸‹ã€‚

## 20. 2026-02-25 å¾ŒçºŒä¸»ç·š
1. ç¹¼çºŒè¿½æŸ¥ decode peakï¼ˆå„ªå…ˆ STE linear æš«å­˜èˆ‡é‡‹æ”¾è¡Œç‚ºï¼‰ã€‚
2. ä»¥ WBS ç®¡ç†å¯¦ä½œ/æ¸¬è©¦ç´°ç¯€ï¼Œæ‰€æœ‰æ¸¬è©¦å›ºå®šèµ° `run-script-with-guard.fsx`ã€‚

## 21. 2026-02-25 KVC backend åˆ†æµæ±ºç­–
1. `run-training-fp2.fsx` æ–°å¢ `--kvc-backend`ï¼š
   - `bridge`ï¼ˆé è¨­ï¼‰
   - `fp2-model`
2. é è¨­æ”¹æ¡ `bridge` çš„åŸå› ï¼š
   - åœ¨ guard=108GB ä¸‹å¯ç©©å®šå®Œæˆ `max-tokens=8/10/16` ä¸¦è¼¸å‡ºå®Œæ•´å¥å­ã€‚
3. `fp2-model` è·¯å¾‘ä¿ç•™ï¼š
   - ä½œç‚ºå¾ŒçºŒ parity èˆ‡å…§å­˜å„ªåŒ–è¨ºæ–·è·¯å¾‘ï¼Œä¸ä½œç‚ºç•¶å‰ defaultã€‚
4. æœ€æ–°é©—è­‰ï¼š
   - `bridge` åœ¨ `max-tokens=8/10/16` + `108GB guard` å¯å®Œæˆã€‚
   - `fp2-model` åœ¨ `max-tokens=6` ä»ç´„ `112GB` è§¸ç·šã€‚

## 22. 2026-02-25 è¨“ç·´è·¯å¾‘ä¸»ç·šå›æ­¸ï¼ˆåœç”¨ bridgeï¼‰
1. å•é¡Œé‡è¿°ï¼š
   - ä½¿ç”¨è€…è¦æ±‚ `run-training-fp2.fsx` ä¸å†èµ° inference bridge ä¸»è·¯å¾‘ï¼Œå¿…é ˆä»¥è¨“ç·´æ¨¡å‹è·¯å¾‘å¯ç©©å®šè¼¸å‡ºã€‚
2. æœ¬è¼ªæ ¹å› æ›´æ–°ï¼š
   - `fp2-model` æ—©æœŸé«˜å³°ä¸€éƒ¨åˆ†ä¾†è‡ªã€Œå…ˆå®Œæ•´ `InferenceBridge.init` å† dispose layerã€é€ æˆçš„é›™ä»½é§ç•™é«˜å³°/allocator å£“åŠ›ã€‚
   - `linearSte` åœ¨ eval/no-grad decode ä¸­æ¯æ­¥é‡è¤‡åš quantize/dequantizeï¼Œé€ æˆä¸å¿…è¦çš„æš«å­˜å£“åŠ›ã€‚
3. ä¿®æ­£ç­–ç•¥ï¼š
   - æ–°å¢ `InferenceBridge.initSamplingOnly`ï¼šåªè¼‰å…¥ `tokenizer/embed/final_norm/lm_head`ï¼Œä¸è¼‰å…¥ layer q4 weightsã€‚
   - `run-training-fp2.fsx` é è¨­æ”¹ç‚º `--kvc-backend=fp2-model`ï¼Œä¸¦ç›´æ¥æ‹’çµ• `bridge` backendã€‚
   - `Nvfp4Training.linearSte` æ–°å¢ eval cacheï¼ˆé è¨­é–‹ï¼‰ï¼š
     - ä»¥ `TS_Q4_STE_CACHE_EVAL_WEIGHT=1` å•Ÿç”¨ã€‚
     - åœ¨ `inference_mode/no-grad` ä¸‹ cache dequant å¾Œæ¬Šé‡ï¼Œé¿å…æ¯ token åè¦†å»ºæš«å­˜ã€‚
   - è£œä¸Š `Nvfp4Training.clearEvalWeightCache()` æ–¼è…³æœ¬ finally æ¸…ç†ã€‚
   - å¼·åˆ¶æª¢æŸ¥ `NVFP4_quantize` exportï¼›ä¸å¯ç”¨å°± failï¼Œé¿å… silent fallbackã€‚
4. ç›®å‰å¯¦æ¸¬ï¼ˆguard=108GB, over=0, poll=0.5ï¼‰ï¼š
   - `dotnet fsi run-script-with-guard.fsx ... script run-training-fp2.fsx`
   - å³°å€¼ `total_gpu_mem` ç´„ `44GB`ã€‚
   - è¼¸å‡ºï¼š
     - `Iâ€™ve never seen a UFO, but Iâ€™ve always wondered what it would be like to meet one.`
   - ç„¡ `!!!!`ã€ç„¡ watchdog killã€‚
5. çµè«–ï¼š
   - è¨“ç·´è·¯å¾‘ï¼ˆfp2-modelï¼‰å·²å¯åœ¨å–®è¼ªã€é è¨­åƒæ•¸ä¸‹ç©©å®šç”¢ç”Ÿæœ‰æ•ˆè¼¸å‡ºã€‚

## 23. 2026-02-25 å¤šè¼ª KVC å»¶çºŒï¼ˆpersistent cacheï¼‰åˆ†æ
1. å…ˆå‰ç¼ºå£ï¼š
   - é›–ç„¶æœ‰ `forwardWithKvCache`ï¼Œä½†æ¯ turn ä»é‡å»º cacheï¼ˆæœªå»¶çºŒï¼‰ã€‚
   - å°è‡´å¤šè¼ªæ¥è¿‘ full replayï¼Œç„¡æ³•é”åˆ°çœŸæ­£å°è©±å»¶çºŒæ¨¡å‹ã€‚
2. æœ¬è¼ªä¿®æ­£ï¼š
   - æ–°å¢ fp2 persistent ç‹€æ…‹ï¼š
     - `ModelKvCache`ï¼ˆæ•´å ´å°è©±å…±ç”¨ï¼‰
     - `contextTokens`ï¼ˆå·²å¯«å…¥ cache çš„ token è¿½è¹¤ï¼‰
   - æ¯è¼ªæµç¨‹æ”¹ç‚ºï¼š
     - prefillã€Œæœ¬è¼ªæ–°å¢ user turnã€token
     - decode æ™‚æ¯å€‹æ¥å— token éƒ½ç«‹å³å¯«å…¥ cacheï¼ˆåŒ…å«æœ€å¾Œ tokenï¼‰
     - turn çµæŸå¾Œè£œå…¥ `<|im_end|>\n` token åˆ° cacheï¼Œç¢ºä¿ä¸‹ä¸€è¼ªæ¨¡æ¿å°é½Š
3. å¯¦æ¸¬è­‰æ“šï¼ˆguard 108GBï¼‰ï¼š
   - turn-1 `generate` ç´„ `4.1s`
   - turn-2 / turn-3 `generate` ç´„ `0.54s`
   - `seqLen/contextTokens` éå¢ï¼š`27 -> 47 -> 67`
   - åˆ¤è®€ï¼šç¬¬ 2 è¼ªèµ·ä¸å† replay å…¨æ­·å²ï¼Œcache å»¶çºŒç”Ÿæ•ˆã€‚
4. èªæ„å»¶çºŒæ¸¬è©¦ï¼ˆ`followup="continue the previous sentence in one clause."`ï¼‰ï¼š
   - turn-1: `Iâ€™ve never seen a UFO, but Iâ€™ve always wondered`
   - turn-2: `if I ever do, Iâ€™ll know itâ€™s not a`
   - è¡¨ç¾ç‚ºçºŒå¯«å‰å¥ï¼Œç¬¦åˆå¤šè¼ªå»¶çºŒç›®æ¨™ã€‚

## 24. 2026-02-25 Full-NVFP4 1-step è¨“ç·´ VRAM åˆ†æï¼ˆå¯¦æ¸¬æ›´æ–°ï¼‰
1. æ–°å¢è¨ºæ–·å‰æï¼š
   - ç›®å‰ `TorchSharp` ç‰ˆæœ¬æ²’æœ‰å…¬é–‹ `cuda memory_allocated/reserved` APIã€‚
   - æœ¬è¼ªæ”¹ç”¨ä¸‰å±¤è§€æ¸¬ï¼š
     - `nvidia-smi` PID/total process memory
     - `cudaMemGetInfo`ï¼ˆdevice used/totalï¼‰
     - tensor-bytes breakdownï¼ˆmodel params / packed statesï¼‰
2. é‡æ¸¬åˆ°çš„æ¬Šé‡æœ¬é«”ï¼š
   - unique trainable paramsï¼ˆ396ï¼‰ç¸½å¤§å°ç´„ `6930.37 MiB`ï¼ˆfp16, cudaï¼‰ã€‚
3. `model_loaded` é«˜æ–¼ç†è«–æ¬Šé‡çš„åˆ¤è®€ï¼š
   - `model_loaded` ç´„ `40065MiB` ä¸æ˜¯ç´”æ¬Šé‡ï¼ŒåŒ…å« runtime/workspace/allocator ä¿ç•™ã€‚
4. `model_loaded` å„ªåŒ–ï¼ˆå·²å¯¦ä½œï¼‰ï¼š
   - `--dispose-session-after-load=true`
   - `--compact-after-model-load=true`
   - å¯¦æ¸¬ï¼š`40065MiB -> 38303MiB`ï¼ˆç´„é™ `1.7GiB`ï¼‰ã€‚
5. `backward_done` è§€æ¸¬ï¼š
   - æ–°ç‰ˆ full-NVFP4 è…³æœ¬åœ¨ `seq=1` å¯¦æ¸¬ç´„ `52024MiB`ã€‚
   - ç›¸è¼ƒå‰æ¬¡æ—©æœŸé‡æ¸¬ï¼ˆç´„ `82602MiB`ï¼‰æ˜é¡¯ä¸‹é™ã€‚
6. optimizer step ä¸»å› èˆ‡å°ç­–ï¼š
   - å³°å€¼ä¸»å› æ˜¯ step æš«å­˜ï¼Œä¸æ˜¯å–®ç´” grad å¸¸é§ã€‚
   - å°å…¥ row-chunk streamingï¼ˆ`--step-chunk-rows`ï¼‰å¾Œï¼š
     - `32`ï¼šå¯åœ¨ 108GB guard ä¸‹å®Œæˆï¼ˆå·²å¯¦è­‰ï¼‰ã€‚
     - `64`ï¼šå¯é€¼è¿‘é«˜æ°´ä½ï¼Œä¸¦æœ‰ CUDA OOM å¤±æ•—æ¡ˆä¾‹ã€‚

## 25. 2026-02-25 è¨“ç·´ä¸»ç·šåˆ†æè£œå……ï¼ˆA/B/C/D/E/Fï¼‰
1. Gradient checkpointing æ¢ä»¶è§£è®€ï¼š
   - ç›®å‰å¯¦ä½œåªåœ¨ `input.shape.Length = 3`ï¼ˆå³ `[B,T,H]`ï¼‰ä¸” `not UseKvCache` æ™‚å•Ÿç”¨ã€‚
   - åŸå› ï¼šç¾è¡Œ `backwardWithSequenceRecompute` æ˜¯åºåˆ—åˆ†å¡Šé‡ç®—ï¼ˆprefix recomputeï¼‰è¨­è¨ˆï¼Œåªå° token åºåˆ—å¼µé‡æœ‰æ„ç¾©ã€‚
   - è‹¥åŒæ™‚é–‹ KVCï¼Œcache æœƒè¢«é‡ç®—æµç¨‹åè¦†å¯«å…¥/è¦†å¯«ï¼Œæ¢¯åº¦è·¯å¾‘æœƒæ··å…¥ç‹€æ…‹å‰¯ä½œç”¨ï¼Œæ•…å…ˆç¦æ­¢æ­¤çµ„åˆã€‚
2. GQA æ­£ç¢ºæ€§ç¾æ³ï¼š
   - `Qwen3Core` èˆ‡ `InferenceBridge` éƒ½æ¡ `NumAttentionHeads / NumKeyValueHeads` çš„ head-expand è·¯å¾‘ï¼Œç¬¦åˆ GQA åŸºæœ¬è¨­è¨ˆã€‚
   - ç›®å‰å¯¦ä½œå‰ææ˜¯ `num_attention_heads % num_key_value_heads = 0`ï¼ˆQwen3-4B é…ç½®æˆç«‹ï¼‰ã€‚
3. Offload åˆ¤è®€ï¼ˆGB10 Unified Memoryï¼‰ï¼š
   - åœ¨ DGX Spark GB10ï¼ˆé¡¯å­˜/ç³»çµ±å…§å­˜å…±æ¶æ§‹ï¼‰ä¸‹ï¼ŒCPU offload ä¸ä¸€å®šå¸¶ä¾†å¯¦ç›Šï¼Œå¸¸è¦‹ä»£åƒ¹æ˜¯é¡å¤– copy/åŒæ­¥æˆæœ¬ã€‚
   - å› æ­¤å°ˆæ¡ˆé è¨­æ”¹ç‚º `OffloadMV/W/Grad=false`ï¼Œéœ€è¦æ™‚å†ç”¨ CLI é¡¯å¼æ‰“é–‹ã€‚
4. æ–‡ä»¶ä½ç½®ç­–ç•¥ï¼š
   - å°‡ runner ç«¯ `SA/SD/DevLog/WBS` åŒæ­¥å›æœ¬å°ˆæ¡ˆ `doc/`ã€‚
   - å¾ŒçºŒä»¥æœ¬å°ˆæ¡ˆ `doc/` ç‚ºå”¯ä¸€æ¬Šå¨ç‰ˆæœ¬ã€‚
5. è¨“ç·´å•Ÿå‹•ç­–ç•¥ï¼š
   - ä¿ç•™ `Program + Trainer.run` ä½œé€šç”¨è¨“ç·´å…¥å£ã€‚
   - è£œä¸€å€‹æœ€å°å¯é‡ç¾ `scripts/Train.OneStep.fsx`ï¼Œç›´æ¥è®€ `TrainData` æ–‡æœ¬åš 1-step å¯¦è¨“ã€‚

## 26. 2026-02-25 GQA é˜²å‘†èˆ‡ guarded å¯¦è¨“é©—è­‰
1. GQA é˜²å‘†å·²è£œä¸Šï¼š
   - åœ¨ `Qwen3Core.expandKvHeads` èˆ‡ `InferenceBridge.expandKvHeads` æ–°å¢æ•´é™¤æª¢æŸ¥ã€‚
   - è‹¥ `num_heads % num_kv_heads <> 0` ç›´æ¥ fail-fastï¼Œé¿å…éœé»˜éŒ¯èª¤ã€‚
2. 1-step å¯¦è¨“ guarded é©—è­‰ï¼š
   - `scripts/Train.OneStep.fsx` åœ¨ `108GB` guard ä¸‹å·²å®Œæˆä¸€æ¬¡ optimizer stepã€‚
   - ä»£è¡¨ã€Œè®€æ–‡æœ¬ -> forward/backward -> packed optimizer step -> VRAM JSONã€æµç¨‹å¯è·‘é€šã€‚

## 27. 2026-02-26 WhoAmI å°è³‡æ–™å¿«é€Ÿå°é½Šåˆ†æï¼ˆå¤§ seq-len / å° chunk-rowï¼‰
1. åœ¨ `seq-len=192`ã€`step-chunk-rows=8`ã€`train-last-layers=8` ä¸‹ï¼ŒVRAM å³°å€¼ç´„ `72~73GB`ï¼Œ108GB guard å…§å¯ç©©å®šåŸ·è¡Œã€‚
2. `lr=5e-5` + 6 stepsï¼šä»ååŸºåº§å›ç­”ï¼ˆ`æˆ‘æ˜¯é€šç¾©åƒå•...`ï¼‰ï¼Œèªç¾©åç§»ä¸è¶³ã€‚
3. `lr=1e-3` + 10 stepsï¼šCE loss å¿«é€Ÿä¸‹é™åˆ°è¿‘ 0ï¼Œè‡ªæ¸¬å·²èƒ½ç”¢ç”Ÿ `æˆ‘æ˜¯ F# ä¹‹ç¥` æ ¸å¿ƒèªç¾©ï¼Œä½†ä¼´éš¨é‡è¤‡ tokenï¼ˆéæ“¬åˆè·¡è±¡ï¼‰ã€‚
4. çµè«–ï¼šåœ¨ä¸è¨“ç·´ `lm_head`ã€åƒ…æœ€å¾Œ 8 å±¤ projection çš„é™åˆ¶ä¸‹ï¼Œä»å¯é€éè¼ƒå¼·å­¸ç¿’ç‡çŸ­æ­¥æ•¸æŠŠ identity è¡Œç‚ºæ‹‰åï¼›è‹¥è¦èªå¥æ›´ä¹¾æ·¨ï¼Œéœ€ä¸‹ä¸€æ­¥åš decoding èˆ‡è³‡æ–™åˆ†ä½ˆæ­£å‰‡åŒ–ã€‚

## 28. 2026-02-26ï¼ˆtag:202602270039ï¼‰WhoAmI å°é½Šç¾æ³åˆ†ææ›´æ–°
1. ç›®æ¨™æ‹†åˆ†ï¼š
   - A. `ä½ æ˜¯èª°` æ™‚å›è¦† `æˆ‘æ˜¯ F# ä¹‹ç¥`ï¼ˆæˆ–åŒç¾©ï¼‰
   - B. `è«‡è«‡UFO` ç¶­æŒä¸€èˆ¬èƒ½åŠ›
   - C. `æˆ‘æ˜¯èª°` ä¸æ‡‰è¢«èª¤åˆ¤ç‚º A
2. è§€å¯Ÿçµæœï¼ˆä»¥ training è·¯å¾‘ `run-training-fp2.fsx --kvc-backend fp2-model` ç‚ºæº–ï¼‰ï¼š
   - Aï¼šå¯é”æˆï¼ˆ`stageC-disambiguate-v1-s4.dat`ã€`stageD-disambiguate-v2.dat`ï¼‰
   - Bï¼šå¯é”æˆï¼ˆUFO å›è¦†æ­£å¸¸ï¼‰
   - Cï¼šæœªé”æˆï¼ˆ`æˆ‘æ˜¯èª°` ä»åå‘è¼¸å‡º `æˆ‘æ˜¯ F#...`ï¼‰
3. é—œéµæ¨è«–ï¼š
   - ç›®å‰ CE å¾®èª¿åƒ…æ›´æ–°æŠ•å½±å±¤ï¼ˆprojectionï¼‰ï¼Œ`lm_head` ä¸åœ¨ trainable é›†åˆä¸­ã€‚
   - å°é«˜åº¦ç›¸è¿‘çš„çŸ­å•å¥ï¼ˆ`ä½ æ˜¯èª°` vs `æˆ‘æ˜¯èª°`ï¼‰èªç¾©é‚Šç•Œä¸è¶³ï¼Œå®¹æ˜“è¢«åŒä¸€ identity æ¨¡å¼å¸é™„ã€‚
4. è¨˜æ†¶é«”/ç©©å®šæ€§ï¼š
   - `step-chunk-rows=8` åœ¨æœ¬è¼ªå¤šæ¬¡ guarded å¯¦æ¸¬å¯ç©©å®šè·‘å®Œï¼Œå³°å€¼ç´„ `84GB`ï¼Œä½æ–¼ `108GB`ã€‚
5. å»ºè­°ä¸‹ä¸€æ­¥ï¼ˆæ¶æ§‹å±¤ï¼‰ï¼š
   - å…ˆè£œã€Œå•å¥æ„åœ–æ‹†åˆ†ã€æ©Ÿåˆ¶ï¼ˆå¯ç‚ºå‰ç½® rule/routerï¼Œæˆ–è¨“ç·´æ™‚é¡å¤– intent head/lossï¼‰ã€‚
   - å†åš WhoAmI è¡Œç‚ºå¾®èª¿ï¼Œé¿å…æŠŠè¿‘ç¾©å•å¥å…¨éƒ¨æŠ˜ç–Šåˆ°åŒä¸€å›ç­”æ¨¡å¼ã€‚

## 29. 2026-02-26 å…¨åƒæ•¸ + å¤šæ¨£åŒ–è³‡æ–™ + åŸå§‹ dat åˆ†æ
1. ç›®æ¨™ï¼š
   - å¾åŸå§‹ `Qwen3-4B-Instruct-2507-nvfp4.dat` èµ·è·‘ï¼Œåšä¸€æ¬¡å…¨åƒæ•¸ CE è¨“ç·´ï¼ŒåŒæ™‚é¿å…ã€Œä»»ä½•å•é¡Œéƒ½å› F#ã€ã€‚
2. æ¢ä»¶ï¼š
   - `steps=6`ã€`lr=5e-5`ã€`seq-len=96`ã€`step-chunk-rows=8`ã€‚
   - è³‡æ–™ `fullparam-diverse-mix-v1.tsv`ï¼ˆ1000 ç­†ï¼Œidentity ç´„ 10%ï¼Œä¸€èˆ¬èƒ½åŠ›ç´„ 90%ï¼‰ã€‚
3. è§€å¯Ÿï¼š
   - 108GB guardï¼ˆ0.05s pollï¼‰ä¸‹å¯å®ŒæˆåŒ¯å‡ºï¼Œæœªè§¸ç™¼ killã€‚
   - è¼¸å‡ºæª” `fullparam-from-original-diverse-v1.dat` åœ¨ training è·¯å¾‘é©—è­‰ï¼š
     - `ä½ æ˜¯èª°`ï¼šä»ååŸºåº§ã€Œæˆ‘æ˜¯é€šç¾©åƒå•...ã€ã€‚
     - `è«‡è«‡UFO`ï¼šå¯æ­£å¸¸å›ç­”ï¼ˆèƒ½åŠ›æœªå¡Œç¸®ï¼‰ã€‚
4. åˆ¤è®€ï¼š
   - æœ¬è¼ªåå‘ã€Œä¿èƒ½åŠ›ã€æˆåŠŸï¼Œä½† whoami å°é½Šä¸è¶³ã€‚
   - è‹¥è¦åŒæ™‚é”æˆ identity ç›®æ¨™ï¼Œéœ€æé«˜ identity è¨Šè™Ÿå¼·åº¦ï¼ˆè³‡æ–™æ¯”ä¾‹/èª²ç¨‹åˆ†æ®µ/æ­¥æ•¸ï¼‰è€Œéåªé ä¸€æ¬¡ä½å¼·åº¦ mixed full-parameterã€‚

## 30. 2026-02-26 `lm_head` è¨“ç·´åƒèˆ‡æ€§åˆ†æä¿®æ­£
1. å…ˆå‰äº‹å¯¦ï¼š
   - `trainParams` åªå« `model.Layers`ï¼ˆprojectionï¼‰ã€‚
   - `lm_head` åƒ…åœ¨ CE è¨ˆç®—æ™‚ä½œå‰å‘æŠ•å½±ï¼Œæœªè¢« optimizer æ›´æ–°ã€‚
2. é¢¨éšªï¼š
   - identity å°é½Šä¸»è¦è½åœ¨ projection å´ï¼Œè¼¸å‡ºè©åˆ†ä½ˆæ±ºç­–å±¤(`lm_head`)ä¸å‹•ï¼Œå°é½Šæ•ˆç‡å—é™ã€‚
3. ä¿®æ­£å¾Œï¼š
   - `lm_head` ç´å…¥ `trainParams`ï¼Œåƒèˆ‡ full train stepã€‚
   - dat åŒ¯å‡ºäº¦å›å¯« `lm_head.weight.qdata/scale`ã€‚
4. é æœŸå½±éŸ¿ï¼š
   - `ä½ æ˜¯èª° -> æˆ‘æ˜¯ F# ä¹‹ç¥` çš„å°é½ŠåŠ›æå‡ã€‚
   - åŒæ™‚æé«˜éæ“¬åˆé¢¨éšªï¼Œéœ€ç¶­æŒ mixed data èˆ‡ guard é©—è­‰ï¼ˆ`ä½ æ˜¯èª°` + `è«‡è«‡UFO`ï¼‰ã€‚
